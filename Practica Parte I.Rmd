---
title: "Práctica AE"
author: "Antonio Galián Gálvez"
date: "2024-10-28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(encoding = "UTF-8")
```

```{r}
# Cargamos las librerías
library(gplots)
library(corrplot)
library(glmnet)
library(MASS)
library(pls)
```


# 0. Cargamos los datos y eliminamos la columna train
```{r}
# Cargamos los datos con separador de tabulador
datos <- read.delim("prostate.data.txt", header = TRUE, sep = "\t")

# Eliminamos la columna train
datos <- datos[, -ncol(datos)]
```


# 1. Exploración de datos
**¿Cuántas variables hay?**
```{r}
# Vemos las variables que hay
ncol(datos)
```
Vemos que hay 10 variables.


**¿De qué clase son?**
```{r}
# Vemos la clase de las variables
str(datos)
```
Vemos que son variables numéricas.

**¿Hay una variable que correspondiente al identificador de paciente? Si es así, elimínala.**
```{r}
# Como hay una variable Id, llamada "X", la eliminamos
datos <- datos[, -1]
```
La variable $X$ era la Id de los pacientes. Tras eliminarla quedan 9 varaibles.

**¿Hay valores nulos en alguno de los ficheros?**
```{r}
# Comprobamos si hay NA
sum(is.na(datos))
```
No hay valores nulos.

**¿Están estandarizadas las variables? En este punto del análisis, ¿es necesario normalizarlas?**
```{r}
# Comprobamos si las variables estan estandarizadas
summary(datos)
```
Vemos que las variables no están estandarizadas, ya que no tienen media 0 ni desviación 1. 
No hace falta normalizarlas en este caso.

**¿Por qué crees que algunas variables están es escala logarítmica?**

Algunas variables pueden estar en escala logarítmica para reducir 
relaciones de potencias y exponenciales entre dos variables a relaciones lineales,
para así poder realizar regresiones lineales. Otra razón puede ser que estas variables
tomen valores de órdenes de magnitud muy diferentes, simplificando su tratamiento al 
hacer el logaritmo.


# 2. Análisis de variable categóricas
```{r}
# Convertimos las variables en factores
datos$svi <- as.factor(datos$svi)
datos$gleason <- as.factor(datos$gleason)
datos$age <- as.factor(datos$age)

# Hacemos attach a los datos
attach(datos)

# Comprobamos que las variables son categóricas
str(datos)

# Comprobamos la dispersión de sus valores
par(mfrow = c(3, 1))
plot(svi, main = "SVI", ylab = "Frecuencia")
plot(gleason, main = "Gleason", ylab = "Frecuencia")
plot(age, main = "Age", ylab = "Frecuencia")
par(mfrow = c(1, 1))
```

Podemos ver en los gráficos anteriores las tendencias de agrupamiento en los valores de las variables. 

- En SVI la mayoría de los datos se concentran en SVI = 0.
- En Gleason la mayoría de los datos se acumulan en 7, mientras que apenas tenemos muestra en 8.
- En Age las datos se agrupan alrededor 64 y disminuyen al alejarse, aunque el valor 68 dispone de una cantidad de muestras por encima de lo normal.


# 3. Análisis de frecuencias

**¿Qué porcentaje de pacientes con la puntuación de Gleason igual a 7, presenta índice igual svi igual a 0?**
```{r}
# Seleccionamos los pacientes con la puntuación de Gleason igual a 7 y los que tienen svi igual a 0 dentro de estos
datos.gleason7 <- datos[datos$gleason == "7", ]
datos.gleason7.svi0 <- datos.gleason7[datos.gleason7$svi == "0", ]

# Vemos los pacientes que hay en datos.gleason.7.svi0 y en gleason7
patients.gleason7.svi0 <- nrow(datos.gleason7.svi0)
patients.gleason7 <- nrow(datos.gleason7)

# Dividimos la cantidad de pacientes filtrados entre el total
porcentaje <- patients.gleason7.svi0 / patients.gleason7 * 100 # creo que hay que dividir entre el los pacientes con gleason7, no los totales
porcentaje
```
Vemos que el porcentaje es del 66.07143%.


**¿Qué porcentaje de pacientes con índice svi igual a 0 tiene la puntuación de Gleason igual a 7?**
```{r}
# Seleccionamos los individuos con svi igual a 0 y con gleason igual a 7 dentro de estos
datos.svi0 <- datos[datos$svi == "0", ]
datos.svi0.gleason7 <- datos.svi0[datos.svi0$gleason == "7", ]

# Vemos los pacientes que hay en datos.svi0
patients.svi0 <- nrow(datos.svi0)

# Hacemos el porcentaje
porcentaje <- patients.gleason7.svi0 / patients.svi0 * 100
porcentaje
```
Vemos que el porcentaje es del 48.68421%.


**Estas dos variables, ¿son independientes?**
```{r}
# Creamos una tabla con las dos variables
tabla <- table(svi, gleason)

# Creamos tablas de probabilidad por fila y por columna
addmargins(prop.table(tabla, 1), 2) * 100
addmargins(prop.table(tabla, 2), 1) * 100

# Realizamos un gráfico de la tabla para visualizar mejor la independencia
heatmap.2(
    prop.table(tabla),
    xlab = "Gleason", ylab = "SVI",
    density.info = "none",
    col = blues9,
    trace = "none"
)
```
Se puede ver en la gráfica que la mayoría de los casos se acumulan en zonas concretas:

- Cuando SVI es 0, se acumulan en Gleason = 6 y 7.
- Cuando SVI es 1, se acumulan en Gleason = 7.

Por lo tanto, como los datos no se distribuyen por igual en todos los casos, las dos variables son dependientes.


# 4. Regresión lineal simple

**El plot de los datos junto a la recta de regresión.**
```{r}
# Realizamos el modelo lineal
recta <- lm(lpsa ~ lcavol)

# Representamos el modelo sobre los datos
plot(lcavol, lpsa, main = "lpsa vs lcavol")
abline(recta, col = "red")
```

**Intervalos de confianza para los coeficientes del modelo con una confianza de 0.95.**
```{r}
# Realizamos el intervalo de confianza al 95%
intervals <- confint(recta, level = 0.95)
intervals
```

**Definición de RSE y su valor.**

Por definición, el $RSE$ es la suma de los cuadrados de los residuos:
$$
RSE = \sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{n - 2}}
$$
donde $y_i - \hat{y_i}$ son los residuos.
```{r}
# Calculamos la suma de cuadrados de los residuos (RSE)
r1 <- residuals(recta)
RSE <- sqrt(sum(r1^2) / (dim(datos)[1] - 2))
RSE
```
EL RSE vale 0.7874996.

**Estudio de la eficacia del modelo.**
```{r}
# Vemos el resumen del modelo
recta.summary <- summary(recta)
recta.summary

# Calculamos el porcentaje de variación relativo de los intervalos de confianza respecto al valor predicho de los coeficientes
abs(intervals[1, 1] - intervals[1, 2]) / recta.summary$coefficients[1, 1] * 100
abs(intervals[2, 1] - intervals[2, 2]) / recta.summary$coefficients[2, 1] * 100

# Calculamos el porcentaje de error que hay respecto a la media de lpsa
RSE / mean(lpsa) * 100
```
Se puede ver en el modelo lineal que los p-value toman valores muy pequeños (menores a 2e-16). 
Además, el t-value toma valores mayores a 10 (10.55 y 12.36). Estas dos cosas parecen indicarnos
que los coeficientes del modelo lineal no son nulos, por lo que las variables están relacionadas.

Sin embargo, el valor $R^2$ vale 0.5394, por lo que solo el 53.94% de la variación de lpsa es explicada por lcavol.
El RSE es de 0.7875, que al compararlo con la media de lpsa, vemos que hay un 31.77% de error, que no es poco.
Además, los intervalos de confianza son:

- (1.2652222, 1.7493727) para el valor de $\beta_0 = 1.50730$.
- (0.5839404, 0.8547004) para el valor de $\beta_1 = 0.71932$.

Si comparamos estos intervalos de confianza con los valores predichos de los coeficientes:

- Para el valor $\beta_0$ hay una variación del 32.12%.
- Para el valor $\beta_1$ hay una variación del 37.64%.

En resumen, los valores de p-value y t-value nos indican que las variables efectivamente tienen relación.
Sin embargo, el resto de resultados del análisis nos dicen que el modelo lineal no es muy bueno para este caso,
y que no tiene un gran desempeño.

**Interpretación. El modelo lineal calculado, ¿cómo lo interpretas? Concretamente, ¿cómo a través del modelo lineal llegas a otro que relacionan las variables cavol y psa (sin los log. neperianos)?**

El modelo lineal tiene una pendietne positiva, lo que nos indica que la variable lpsa aumentará junto con lcavol.
Dado que estas variables son logaritmos, podemos obtener una relación no lineal entre ellas haciendo la exponencial.
$$
    \ln(psa) = \beta_0 + \beta_1 \ln(cavol) \Rightarrow psa = e^{\beta_0 + \beta_1 \ln(cavol)} = e^{\beta_0} cavol^{\beta_1}
$$
Podemos ver como a partir de la relación entre los logaritmos se ha obtenido una relación de potencias.
En esta relación, la variable psa será una potencia de cavol con exponente $\beta_1$, y la variable $\beta_0$ 
intervendrá en la proporcionalidad de estas psa y la potencia de cavol. Como $\beta_0 > 0$, la constante de 
proporcionalidad será mayor que 1, y como $\beta_1$ está entre 0 y 1, la relación te potencias será como
una raíz de valor $1/\beta_1 \approx 1.4$.


# 5. Regresión lineal multiple
```{r}
# Seleccionamos las columnas numéricas
num_cols <- which(sapply(datos, is.numeric))

# Hacemos un plot de la matriz de correlación
corrplot::corrplot(cor(datos[, num_cols]), type = "upper", tl.cex = 0.5)

# Cremos un dataframe con solo los datos numéricos
datos.num <- datos[, c(-3, -7, -8)]

# Realizamos un modelo lineal entre lpsa y las variables numéricas
rectaMul <- lm(lpsa ~ ., data = datos.num)

# Vemos el modelo lineal
summary(rectaMul)
```

Se puede ver en la matriz de correlación como algunas variables parecen tener cierta dependencia.
En el caso de lpsa, parece estar relacionada con lcavol, lweight, lcp y en menor medida con pgg45. 
Si analizamos el p-value de los coeficientes, podemos ver que solo las variables svi1, lweight y 
lcavol tienen valores bajos, acompañados de t-values relativamente altos. 

Sin embargo, los valores de $R^2$ y RSE son mejores que en el modelo lineal simple con lcavol, 
siendo de 0.6443 y 0.7071 respectivamente. Por lo tanto, podemos determinar que lpsa está relacionada 
con algunas variables y que el modelo lineal múltiple da mejores resultados que el simple, pero 
siguen siendo resultados con grandes errores. Además, se podrían eliminar algunas variables 
con las cuales no parece haber dependencia.


# 6. Modelo de Ridge y Lasso

**Realizamos el modelo Ridge**
```{r}
# Seleccionamos los datos para realizar los modelos
x <- model.matrix(lpsa ~ ., datos.num)[, -1]
y <- datos.num$lpsa

# Seleccionamos la semilla para los números aleatorios
set.seed(1)

# Seleccionamos los conjuntos de entrenamiento y test
train <- sample(seq(1, nrow(x)), nrow(x) / 2) # conjunto de entrenamiento
test <- (-train) # conjunto de testeo

# Guardamos los conjuntos de entrenamiento y test en variables
x.train <- x[train, ]
x.test <- x[test, ]
y.test <- y[test]
y.train <- y[train]

# Hacemos una malla con los valores de lambda
malla <- 10^seq(10, -2, length = 100)
malla.ridge.train <- glmnet(x.train, y.train, alpha = 0, lambda = malla) # regresion ridge sin CV con conjunto de entrenamiento

# Representamos los valores de los coeficientes a lo largo del valor lambda
plot(malla.ridge.train, xvar = "lambda")
legend("topright", lty = 1, col = 2:ncol(datos.num) - 1, legend = names(datos.num[-ncol(datos.num)]))

# Realizamos una regresión Ridge con CV
cv.out.ridge.train <- cv.glmnet(x.train, y.train, alpha = 0)

# Seleccionamos el mejor lambda
bestlam.ridge.train <- cv.out.ridge.train$lambda.min
bestlam.ridge.train

# Realizamos la regresión Ridge con CV y el mejor lambda
ridge.train <- glmnet(x.train, y.train, alpha = 0, lambda = bestlam.ridge.train)
coef(ridge.train)[, 1]

# Realizamos una regresión múltiple con los datos de entrenamiento
rectaMul.train <- glmnet(x.train, y.train, alpha = 0, lambda = 0)
coef(rectaMul.train)[, 1]

# Realizamos una gráfica para ver el MSE de los ajustes para cada valor de lambda
plot(cv.out.ridge.train)

# Hacemos una predicción de Ridge para sacar los valores del MSE
ridge.pred <- predict(ridge.train, newx = x.test) # predicion de 'ridge.train' en conjunto de testeo
ridge.MSE <- mean((ridge.pred - y.test)^2) # MSE estimado de Ridge

# Hacemos una predicción el modelo de regresión lineal múltiple para sacar los valores del MSE
rectaMul.pred <- predict(rectaMul.train, newx = x[test, ]) # predicion de 'rectaMul.train' en conjunto de testeo
rectMul.MSE <- mean((rectaMul.pred - y.test)^2) # MSE estimado de rectaMul

# Comparamos los valores de los MSE con las medias de los valores de testeo
ridge.MSE / mean(y.test) * 100
rectMul.MSE / mean(y.test) * 100
```
 
Tras realizar la validación cruzada, podemos ver como el mejor $\lambda$ es $\lambda = 0.1997304$. 

Al comparar el modelo Ridge con el $\lambda$ óptimo y el modelo de regresión lineal múltiple, 
se observa cómo el valor de MSE es menor para el modelo de regresión Ridge. Comparando estos 
valores con la media de los valores del conjunto de testeo, se observa una mejora de 
aproximadamente un 1.4% (del 20.7% al 19.3%).


**Realizamos el modelo Lasso**
```{r}
# Realizamos una regresión LASSO sin CV para el conjunto de entrenamiento
malla.lasso.train <- glmnet(x.train, y.train, alpha = 1, lambda = malla)

# Representamos los valores de los coeficientes a lo largo del valor lambda
plot(malla.lasso.train, xvar = "lambda")
legend("topright", lty = 1, col = 2:ncol(datos.num) - 1, legend = names(datos.num[-ncol(datos.num)]))

# Realizamos una regresión LASSO con CV para el conjunto de entrenamiento
cv.out.lasso.train <- cv.glmnet(x[train, ], y[train], alpha = 1)

# Seleccionamos el mejor lambda (el que minimiza el MSE)
bestlam.lasso.train <- cv.out.lasso.train$lambda.min
bestlam.lasso.train

# Realizamos una regresión LASSO con el mejor lambda para el conjunto de entrenamiento
lasso.train <- glmnet(x[train, ], y[train], alpha = 1, lambda = bestlam.lasso.train)
coef(lasso.train)[, 1]

# Realizamos una gráfica para ver el MSE de los ajustes para cada valor de lambda
plot(cv.out.lasso.train) # MSE vs log(lambda)

# Hacemos una predicción del LASSO para sacar los valores del MSE
lasso.pred <- predict(lasso.train, newx = x[test, ]) # predicion de 'lasso.train' en conjunto de testeo
lasso.MSE <- mean((lasso.pred - y.test)^2) # MSE estimado de lasso
lasso.MSE
lasso.MSE / mean(y.test) * 100

# Hacemos una predicción el modelo de regresión lineal múltiple para sacar los valores del MSE
rectaMul.pred <- predict(rectaMul.train, newx = x[test, ])
rectaMul.MSE <- mean((rectaMul.pred - y.test)^2) # MSE estimado de rectaMul
rectaMul.MSE
rectaMul.MSE / mean(y.test) * 100
```

Podemos observar que el mejor $\lambda$ para realizar el ajuste es $\lambda = 0.04833733$. 

Al comparar el modelo LASSO con el $\lambda$ óptimo y el modelo de regresión lineal múltiple, 
se observa cómo el valor de MSE es menor para el modelo de regresión LASSO. Comparando estos valores 
con la media de los valores del conjunto de testeo, se observa una mejora de aproximadamente 
un 1.2% (del 20.7% al 19.5%).

Se puede apreciar como Ridge ha dado un valor de MSE un poco menor que LASSO.
Sin embargo, ambos modelos tienen un buen desempeño con resultados muy similares.


# 7. LDA
```{r}
# Realizamos el modelo LDA
lda <- lda(svi ~ lcavol + lcp + lpsa)
lda

# Realizamos una predicción sobre el resultado de LDA
lda.pred <- predict(lda)

# Realizamos una tabla de confusión con LDA y SVI
predicted <- lda.pred$class
tabla.confusion.lda <- table(predicted, svi) # tabla de confusion
tabla.confusion.lda <- round(addmargins(prop.table(tabla.confusion.lda)) * 100, 2)
tabla.confusion.lda

# Valores acertados
tabla.confusion.lda[1, 1] / tabla.confusion.lda[3, 1] * 100 # Valores acertados de SVI = 0
tabla.confusion.lda[2, 2] / tabla.confusion.lda[3, 2] * 100 # Valores acertados de SVI = 1

# Valores errados
tabla.confusion.lda[1, 2] / tabla.confusion.lda[1, 3] * 100 # Valores errados de SVI = 0
tabla.confusion.lda[2, 1] / tabla.confusion.lda[2, 3] * 100 # Valores errados de SVI = 1
```

Se puede ver en los análisis de la tabla de confusión que el modelo LDA acierta:

- Un 90.8% de las veces cuando SVI = 0.
- Un 81.0% de las veces cuando SVI = 1.
  
Por otro lado, se ve que el modelo LDA falla:

- Un 5.5% de las veces cuando SVI = 0.
- Un 29.2% de las veces cuando SVI = 1.

Por lo tanto, es un buen modelo a la hora de acertar, pero tiene una 
gran probabilidad de falso positivo en SVI = 1, prácticamente del 30%.


# 8. Regresión Logística
```{r}
# Realizamos la regresión logística
lr <- glm(svi ~ lcavol + lcp + lpsa, family = binomial)

# Calculamos las probabilidades del modelo
lr.probs <- predict(lr, type = "response")

# Calculamos las predicciones del modelo
lr.pred <- rep(0, 97)
lr.pred[lr.probs > .5] <- 1

# Hacemos las tablas
tabla.confusion.lr <- prop.table(table(lr.pred, svi)) * 100
tabla.confusion.lr <- round(addmargins(tabla.confusion.lr), 2)
tabla.confusion.lr

# Realizamos los análisis de la tabla de confusión
tabla.confusion.lr[1, 1] / tabla.confusion.lr[3, 1] * 100 # Valores acertados de SVI = 0
tabla.confusion.lr[2, 2] / tabla.confusion.lr[3, 2] * 100 # Valores acertados de SVI = 1
tabla.confusion.lr[1, 2] / tabla.confusion.lr[1, 3] * 100 # Valores errados de SVI = 0
tabla.confusion.lr[2, 1] / tabla.confusion.lr[2, 3] * 100 # Valores errados de SVI = 1

# Probabilidad de svi = 1 con lcabol = 2.8269, lcp = 1.843, lpsa = 3.285
predict(lr, newdata = data.frame(lcavol = 2.8269, lcp = 1.843, lpsa = 3.285), type = "response")
```

El modelo de regresión logística acierta:

- Un 96.1% de las veces cuando SVI = 0.
- Un 71.4% de las veces cuando SVI = 1.
  
Por otro lado, el modelo de regresión logística falla:

- Un 7.6% de las veces cuando SVI = 0.
- Un 16.6% de las veces cuando SVI = 1.

Se puede apreciar como el modelo es bueno para acertar en SVI = 0, y 
relativamente bueno para acertar en SVI = 1. Además, tiene mucho menos 
probabilidad de falso positivo que el modelo LDA, teniendo una 
probabilidad del 7.6% de falso positivo en SVI = 0 y 16.6% en SVI = 1.

La probabilidad de SVI = 1 con lcavol = 2.8269, lcp = 1.843 y lpsa = 3.285 es del 77.1%.


# 9. PCA-PCR

**Un biplot y summary del modelo. Coméntalos.**
```{r}
# Convertimos los datos SVI en numéricos
datos.num$svi <- as.numeric(datos$svi)

# Seleccionamos las variables con las que hacer el PCA
variables_pca <- c("lcavol", "lweight", "lbph", "lcp", "svi")

# Seleccionamos los datos con las variables para el PCA
datos_pca <- datos.num[, variables_pca]

# Realizamos el PCA
pr.out <- prcomp(datos_pca, scale = TRUE)

# Representamos el PCA con un biplot
biplot(pr.out, scale = 0)

# Hacemos un resumen del PCA
summary(pr.out)
```

En el biplot vemos dos grupos de flechas. Vemos que los vectores de 
lbph y de lweight están relativamente cerca, apuntando hacia la direción de 
PC2 con unos grados de desviación en sentido horario. Por otro lado, tenemos el grupo 
de lcavol, lcp y svi, que estan cerca unos de otros apuntando positivamente en el
eje de PC1, con unos grados de desviación en sentido horario. La variable que más
alineada está con PC1 es lcavol, y la más alineada con PC2 es lbph. Que varios vectores
de variables estén juntos indican que estas están correlacionadas entre sí, por lo que
lpbh y lweight tinen relación entre sí, al igual que lcavol, lcp y svi, pero estos dos 
grupos no están correlacionados.

Si analizamos los resultados del `summary`, vemos que PC1 explica el 47% de la varianza,
y que PC2 explica el 28%, siendo por tanto las variables más importantes del PCA.
Con estas dos variables se puede explicar el 75% de la varianza. El hecho de que PC1 
explique casi la mitad de la varianza, hace que las variables que están relacionadas 
con PC1 (lcavol, lcp y svi) sean las más importantes (sobre todo lcavol). 


**Justificar la desviación estándar del primer componente principal.**
```{r}
# Representamos la matriz de correlación con los datos del PCA
corrplot::corrplot(cor(datos_pca), type = "upper", tl.cex = 0.5)

# Mostramos la desviación estándar del PCA
pr.out$sdev

# Calculamos la proporcion de la varianza de la componente PC1
1.5341^2 / (1.5341^2 + 1.1862^2 + 0.7258^2 + 0.66771^2 + 0.51651^2)
```
Se puede ver como el cálculo de la desviación estándar de la componente PC1 es correcto.
Como se menciona anteriormente, esta componente explica casi el 50% de toda la varianza.

Si ahora nos fijamos en la matriz de correlación, vemos como efectivamente las variables
lcavol, lcp y svi tienen una correlación alta. Como estas tres variables están áltamente
correlacionadas y apuntan en la dirección de PC1, no es de extrañar que la varianza de PC1
sea la más alta.

**Tras la proporción de varianza acumulada, ¿cuáles son las componentes principales que reflejan el 80% de la varianza total de los datos?**
```{r}
# Hacemos un resumen del PCA
summary(pr.out)
```
Se puede ver en el resumen como las tres primeras componentes principales
(PC1, PC2 y PC3) reflejan más del 80% de la varianza total de los datos (el 85.75%).


También se pueden observar estas proporciones de manera más visual.

```{r}

# Proportion of Variance Explained
pve <-  pr.out$sdev^2/sum(pr.out$sdev^2)

# Creamos dos gráficas
par(mfrow = c(1, 2))

# Gráfica con Proportion of Variance Explained 
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")

# Gráfica con Cumulative Proportion of Variance Explained 
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
abline(h = 0.8,col = 2)

```
Se aprecia también en la segunda gráfica que son los tres primeros componentes principales los que captan más del 80% de la varianza total de los datos.

**Además, aplica PCR para predecir lpsa teniendo en cuenta las variables lcavol, lweight, lbph, lcp, svi. ¿Qué conclusiones podrías sacar?¿Este modelo es mejor que el del apartado 5?**
```{r}
# Creamos variables con los datos
x.pcr <- model.matrix(lpsa ~ ., datos.num)[, -1] # datos de entrada
y.pcr <- datos.num$lpsa # datos de salida

# Creamos un conjunto de testeo
y.pcr.test <- y.pcr[test]

# Hacemos una regresión de componentes principales (PCR) con el conjunto de entrenamiento
pcr.fit <- pcr(lpsa ~ ., data = datos.num, subset = train, scale = TRUE, validation = "CV")

# Hacemos un resumen de la regresión
summary(pcr.fit)

# Vemos el error en funcion del numero de componentes principales
validationplot(pcr.fit, val.type = "MSEP")

# Hacemos predicciones con el conjunto de testeo y calculamos el MSE
pcr.pred <- predict(pcr.fit, x.pcr[test, ], ncomp = 5) # predecimos con conjunto de test
pcr.MSE <- mean((pcr.pred - y.pcr.test)^2) # comparamos resultado con datos de salida de test

pcr.MSE
ridge.MSE
lasso.MSE
rectaMul.MSE
```
Se puede ver en el gráfico como el error del PCR es menor conforme se aumenta
el número de componentes, siendo su mínimo en 5 componentes. 

Si comparamos el MSE de las predicciones con n = 5 con los modelos anteriores,
vemos que Ridge y LASSO tienen el MSE más bajo (alrededor de 0.46). 
Por otro lado, la regresión múltiple y el PCR tienen valores de MSE más altos,
alrededor de 0.49. 

En conclusión, los modelos que mejor predicen las observaciones son Ridge y LASSO.
Mientras, la regresión múltiple y el PCR tienen resultados muy parecidos, con MSE 
más altos.