---
title: "Práctica AE"
author: "Antonio Galián Gálvez"
date: "2024-10-28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(encoding = "UTF-8")
```

```{r}
# Cargamos las librerías
library(gplots)
library(corrplot)
library(glmnet)
library(MASS)
library(pls)
```


# 0. Cargamos los datos y eliminamos la columna train
```{r}
# Cargamos los datos con separador de tabulador
datos <- read.delim("prostate.data.txt", header = TRUE, sep = "\t")

# Eliminamos la columna train
datos <- datos[, -ncol(datos)]
```


# 1. Exploración de datos
```{r}
# Vemos las variables que hay
ncol(datos)

# Eliminamos la columna id
datos <- datos[, -1]

# Comprobamos si hay NA
sum(is.na(datos))

# Comprobamos si las variables estan estandarizadas
summary(datos)
dim(datos)
names(datos)
str(datos)
```

- Hay 10 variables, 9 si quitamos el id del paciente
- Las variables son numéricas
- La variable correspondiente al identificador del paciente es la primera columna
- No hay valores nulos
- Las variables no están ni normalizadas ni estandarizadas
- Hay variables que están en escala logarítmica ya que algunas variables tienen valores negativos a pesar de estar definidas estrictamente positivas, como la concentración en ng/m


# 2. Análisis de variable categóricas
```{r}
# Convertimos las variables en factores
datos$svi <- as.factor(datos$svi)
datos$gleason <- as.factor(datos$gleason)
datos$age <- as.factor(datos$age)

# Hacemos attach a los datos
attach(datos)

# Comprobamos que las variables son categóricas
str(datos)

# Comprobamos la dispersión de sus valores
par(mfrow = c(3, 1))
plot(svi, main = "SVI")
plot(gleason, main = "Gleason")
plot(age, main = "Age")
par(mfrow = c(1, 1))
```


# 3. Análisis de frecuencias

- **¿Qué porcentaje de pacientes con la puntuación de Gleason igual a 7, presenta índice igual svi igual a 0?**
```{r}
# Seleccionamos los pacientes con la puntuación de Gleason igual a 7 y los que tienen svi igual a 0 dentro de estos
datos.gleason7 <- datos[datos$gleason == "7", ]
datos.gleason7.svi0 <- datos.gleason7[datos.gleason7$svi == "0", ]

######## Modifico esta celda #########################

# Vemos los pacientes que hay en datos.gleason.7.svi0 y en gleason7
patients.gleason7.svi0 <- nrow(datos.gleason7.svi0)
patients.gleason7  <- nrow(datos.gleason7)

# Dividimos la cantidad de pacientes filtrados entre el total
porcentaje <- patients.gleason7.svi0 / patients.gleason7 * 100  # creo que hay que dividir entre el los pacientes con gleason7, no los totales
porcentaje


```
Vemos que el porcentaje es del 66.07143%.


- **¿Qué porcentaje de pacientes con índice svi igual a 0 tiene la puntuación de Gleason igual a 7?**
```{r}
# Seleccionamos los individuos con svi igual a 0 y con gleason igual a 7 dentro de estos
datos.svi0 <- datos[datos$svi == "0", ]
datos.svi0.gleason7 <- datos.svi0[datos.svi0$gleason == "7", ]

###### modifico solamente las variables patients.svi0 ###################

# Vemos los pacientes que hay en datos.svi0
patients.svi0 <- nrow(datos.svi0)


# Hacemos el porcentaje
porcentaje <- patients.gleason7.svi0 / patients.svi0 * 100
porcentaje
```
Vemos que el porcentaje es del 48.68421%.


- **Estas dos variables, ¿son independientes?**
```{r}
# Creamos una tabla con las dos variables
tabla <- table(svi, gleason)

# Creamos tablas de probabilidad por fila y por columna
addmargins(prop.table(tabla, 1), 2) * 100
addmargins(prop.table(tabla, 2), 1) * 100

# Realizamos un gráfico de la tabla para visualizar mejor la Independencia
heatmap.2(
    prop.table(tabla),
    xlab = "Gleason", ylab = "SVI",
    density.info = "none",
    col = blues9,
    trace = "none"
)
```
Se puede ver en la gráfica que la mayoría de los casos se acumulan en zonas concretas:

- Cuando SVI es 0, se acumulan en Gleason = 6 y 7.
- Cuando SVI es 1, se acumulan en Gleason = 7.

Por lo tanto, como los datos no se distribuyen por igual en todos los casos, las dos variables son dependientes.


# 4. Regresión lineal simple
```{r}
# Realizamos el modelo lineal
recta <- lm(lpsa ~ lcavol)

# Vemos el modelo
recta.summary <- summary(recta)
recta.summary

# Representamos el modelo sobre los datos
plot(lcavol, lpsa, main = "lpsa vs lcavol")
abline(recta, col = "red")

# Realizamos el intervalo de confianza al 95%
intervals <- confint(recta, level = 0.95)
intervals

# Calculamos el porcentaje de variación relativo de los intervalos de confianza respecto al valor predicho de los coeficientes
abs(intervals[1, 1] - intervals[1, 2]) / recta.summary$coefficients[1, 1] * 100
abs(intervals[2, 1] - intervals[2, 2]) / recta.summary$coefficients[2, 1] * 100

# Calculamos la suma de cuadrados de los residuos (RSE)
r1 <- residuals(recta)
RSE <- sqrt(sum(r1^2) / (dim(datos)[1] - 2))
RSE / mean(lpsa) * 100
```

Vamos a analizar el modelo lineal:

- El t-value es bastante alto (12.36 y 10.55), por lo que los coeficientes están bastante alejados de ser nulos. 
- El p-value es bastante bajo (<2e-16), lo que refuerza que los coeficientes no son nulos.
- El coeficiente $R^2$ no es muy alto, por lo que quizás el modelo lineal no sea el mejor modelo al que los datos se ajusten. Como el valor es  0.5394, el modelo explica el 53.94% de la variabilidad de lpsa respecto a lcavol.
- El RSE es de 0.7875. Si lo dividimos entre la media de lpsa, vemos que tendríamos un error del 31.77%, lo que indica que el modelo no es muy bueno.
- La lontidud de los intervalos de confiana de las variables representan un 32.1% y 37.6% respecto a los valores estimados de los coeficientes. Esto es una variablilidad importante.

Está claro que las variables lpsa y lcavol están relacionadas. 
Sin embargo, aunque el p-value del ajuste lineal sea bajo, otros factores como el $R^2$, los residuos y los intervalos de confianza nos indican que los datos están muy dispersos respecto al modelo.
En el caso de quedarnos con el modelo, podemos pasar de un modelo lineal a un modelo de potencias, donde cavol es una potencia de psa con exponente $\beta_1$.
$$
    \ln(lpsa) = \beta_0 + \beta_1 \ln(lcavol) \Rightarrow psa = e^{\beta_0 + \beta_1 \ln(lcavol)} = \tilde{\beta_0} cavol^{\beta_1}
$$


# 5. Regresión lineal multiple
```{r}
# Seleccionamos las columnas numéricas
num_cols <- which(sapply(datos, is.numeric))

# Hacemos un plot de la matriz de correlación
corrplot::corrplot(cor(datos[, num_cols]), type = "upper", tl.cex = 0.5)

# Cremos un dataframe con solo los datos numéricos
datos.num <- datos[, c(-3, -7, -8)]

# Realizamos un modelo lineal entre lpsa y las variables numéricas
rectaMul <- lm(lpsa ~ ., data = datos.num)

# Vemos el modelo lineal
summary(rectaMul)
```

Se puede ver en la matriz de correlación como algunas variables parecen tener cierta dependencia.
En el caso de lpsa, parece estar relacionada con lcavol, lweight, lcp y en menor medida con pgg45. 
Si analizamos el p-value de los coeficientes, podemos ver que solo las variables svi1, lweight y lcavol tienen valores bajos, acompañados de t-values relativamente altos. Sin embargo, los valores de $R^2$ y RSE son mejores que en el modelo lineal simple con lcavol, siendo de 0.6443 y 0.7071 respectivamente.
Por lo tanto, podemos determinar que lpsa está relacionada con algunas variables y que el modelo lineal múltiple da mejores resultados que el simple, pero siguen siendo resultados con grandes errores. Además, se podrían eliminar algunas variables con las cuales no parece haber dependencia.

# 6. Modelo de Ridge y Lasso

**Realizamos el modelo Rigde**
```{r}
# Seleccionamos los datos para realizar los modelos
x <- model.matrix(lpsa ~ ., datos.num)[, -1]
y <- datos.num$lpsa

# Seleccionamos la semilla para los números aleatorios
set.seed(1)

# Seleccionamos los conjuntos de entrenamiento y test
train <- sample(seq(1, nrow(x)), nrow(x) / 2) # conjunto de entrenamiento
test <- (-train) # conjunto de testeo

# Guardamos los conjuntos de entrenamiento y test en variables
x.train <- x[train, ]
x.test <- x[test, ]
y.test <- y[test]
y.train <- y[train]

# Hacemos una malla con los valore de lambda
malla <- 10^seq(10, -2, length = 100)
malla.ridge.train <- glmnet(x.train, y.train, alpha = 0, lambda = malla) # regresion ridge sin CV con conjunto de entrenamiento

# Representamos los valores de los coeficientes a lo largo del valor lambda
plot(malla.ridge.train, xvar = "lambda")
legend("topright", lty = 1, col = 2:ncol(datos.num)-1, legend = names(datos.num[-ncol(datos.num)]))

# Realizamos una regresión Ridge con CV
cv.out.ridge.train <- cv.glmnet(x.train, y.train, alpha = 0)

# Seleccionamos el mejor lambda
bestlam.ridge.train <- cv.out.ridge.train$lambda.min
bestlam.ridge.train

# Realizamos la regresión Ridge con CV y el mejor lambda
ridge.train <- glmnet(x.train, y.train, alpha = 0, lambda = bestlam.ridge.train)
coef(ridge.train)[, 1]

# Realizamos una regresión múltiple con los datos de entrenamiento
rectaMul.train <- glmnet(x.train, y.train, alpha = 0, lambda = 0) 
coef(rectaMul.train)[, 1]

# Realizamos una gráfica para ver el MSE de los ajustes para cada valor de lambda
plot(cv.out.ridge.train)

# Hacemos una predicción del Ridge para sacar los valores del MSE
ridge.pred <- predict(ridge.train, newx = x.test) # predicion de 'ridge.train' en conjunto de testeo
ridge.MSE <- mean((ridge.pred - y.test)^2) # MSE estimado de Ridge

# Hacemos una predicción el modelo de regresión lineal múltiple para sacar los valores del MSE
rectaMul.pred <- predict(rectaMul.train, newx = x[test, ]) # predicion de 'rectaMul.train' en conjunto de testeo
rectMul.MSE <- mean((rectaMul.pred - y.test)^2) # MSE estimado de rectaMul

# Comparamos los valores de los MSE con las medias de los valores de testeo
ridge.MSE / mean(y.test) * 100
rectMul.MSE / mean(y.test) * 100
```
 
Tras realizar la validación cruzada, podemos ver como el mejor $\lambda$ es $\lambda = 0.1997304$. 
Al comparar el modelo Ridge con el $\lambda$ óptimo y el modelo de regresión lineal múltiple, se observa cómo el valor de MSE es menor para el modelo de regresión Ridge. Comparando estos valores con la media de los valores del conjunto de testeo, se observa una mejora de aproximadamente un 1.4% (del 20.7% al 19.3%).


**Realizamos el modelo Lasso**
```{r}
# Realizamos una regresión LASSO sin CV para el conjunto de entrenamiento
malla.lasso.train <- glmnet(x.train, y.train, alpha = 1, lambda = malla)

# Representamos los valores de los coeficientes a lo largo del valor lambda
plot(malla.lasso.train, xvar = "lambda")
legend("topright", lty = 1, col = 2:ncol(datos.num) - 1, legend = names(datos.num[-ncol(datos.num)]))

# Realizamos una regresión LASSO con CV para el conjunto de entrenamiento
cv.out.lasso.train <- cv.glmnet(x[train, ], y[train], alpha = 1)

# Seleccionamos el mejor lambda (el que minimiza el MSE)
bestlam.lasso.train <- cv.out.lasso.train$lambda.min
bestlam.lasso.train

# Realizamos una regresión LASSO con el mejor lambda para el conjunto de entrenamiento
lasso.train <- glmnet(x[train, ], y[train], alpha = 1, lambda = bestlam.lasso.train)
coef(lasso.train)[, 1]

# Realizamos una gráfica para ver el MSE de los ajustes para cada valor de lambda
plot(cv.out.lasso.train) # MSE vs log(lambda)

# Hacemos una predicción del LASSO para sacar los valores del MSE
lasso.pred <- predict(lasso.train, newx = x[test, ]) # predicion de 'lasso.train' en conjunto de testeo
lasso.MSE <- mean((lasso.pred - y.test)^2) # MSE estimado de lasso
lasso.MSE
lasso.MSE / mean(y.test) * 100

# Hacemos una predicción el modelo de regresión lineal múltiple para sacar los valores del MSE
rectaMul.pred <- predict(rectaMul.train, newx = x[test, ])
rectaMul.MSE <- mean((rectaMul.pred - y.test)^2) # MSE estimado de rectaMul
rectaMul.MSE
rectaMul.MSE / mean(y.test) * 100
```

Podemos observar que el mejor $\lambda$ para realizar el ajuste es $\lambda = 0.04833733$. 
Al comparar el modelo LASSO con el $\lambda$ óptimo y el modelo de regresión lineal múltiple, se observa cómo el valor de MSE es menor para el modelo de regresión LASSO. Comparando estos valores con la media de los valores del conjunto de testeo, se observa una mejora de aproximadamente un 1.2% (del 20.7% al 19.5%).

Se puede ver como Ridge ha dado un valor de MSE un poco menor que LASSO, pero son resultados muy similares.


# 7. LDA
```{r}
# Realizamos el modelo LDA
lda <- lda(svi ~ lcavol + lcp + lpsa)
lda

# Realizamos una predicción sobre el resultado de LDA
lda.pred <- predict(lda)

# Realizamos una tabla de confusión con LDA y SVI
predicted <- lda.pred$class
tabla.confusion.lda <- table(predicted, svi) # tabla de confusion
tabla.confusion.lda <- round(addmargins(prop.table(tabla.confusion.lda)) * 100, 2)
tabla.confusion.lda

# Valores acertados
tabla.confusion.lda[1,1] / tabla.confusion.lda[3,1] * 100 # Valores acertados de SVI = 0
tabla.confusion.lda[2,2] / tabla.confusion.lda[3,2] * 100 # Valores acertados de SVI = 1

# Valores errados
tabla.confusion.lda[1,2] / tabla.confusion.lda[1,3] * 100 # Valores errados de SVI = 0
tabla.confusion.lda[2,1] / tabla.confusion.lda[2,3] * 100 # Valores errados de SVI = 1
```

Se puede ver en los análisis de la tabla de confusión que el modelo LDA acierta:

- Un 90.8% de las veces cuando SVI = 0.
- Un 81.0% de las veces cuando SVI = 1.
  
Por otro lado, se ve que el modelo LDA falla:

- Un 5.5% de las veces cuando SVI = 0.
- Un 29.2% de las veces cuando SVI = 1.

Por lo tanto, es un buen modelo a la hora de acertar, pero tiene una gran probabilidad de falso positivo en SVI = 1, prácticamente del 30%.


# 8. Regresión Logística
```{r}
# Realizamos la regresión logística
lr <- glm(svi ~ lcavol + lcp + lpsa, family = binomial)

# Calculamos las probabilidades del modelo
lr.probs <- predict(lr, type = "response")

# Calculamos las predicciones del modelo
lr.pred <- rep(0, 97)
lr.pred[lr.probs > .5] <- 1

# Hacemos las tablas
tabla.confusion.lr <- prop.table(table(lr.pred, svi)) * 100
tabla.confusion.lr <- round(addmargins(tabla.confusion.lr), 2)
tabla.confusion.lr

# Realizamos los análisis de la tabla de confusión
tabla.confusion.lr[1,1] / tabla.confusion.lr[3,1] * 100 # Valores acertados de SVI = 0
tabla.confusion.lr[2,2] / tabla.confusion.lr[3,2] * 100 # Valores acertados de SVI = 1
tabla.confusion.lr[1,2] / tabla.confusion.lr[1,3] * 100 # Valores errados de SVI = 0
tabla.confusion.lr[2,1] / tabla.confusion.lr[2,3] * 100 # Valores errados de SVI = 1

# Probabilidad de svi = 1 con lcabol = 2.8269, lcp = 1.843, lpsa = 3.285
predict(lr, newdata = data.frame(lcavol = 2.8269, lcp = 1.843, lpsa = 3.285), type = "response")
```

El modelo de regresión logística acierta:

- Un 96.1% de las veces cuando SVI = 0.
- Un 71.4% de las veces cuando SVI = 1.
  
Por otro lado, el modelo de regresión logística falla:

- Un 7.6% de las veces cuando SVI = 0.
- Un 16.6% de las veces cuando SVI = 1.

Se puede apreciar como el modelo es bueno para acertar en SVI = 0, y relativamente bueno para acertar en SVI = 1. Además, tiene mucho menos probabilidad de falso positivo que el modelo LDA, teniendo una probabilidad del 7% de falso positivo en SVI = 0 y 16.6% en SVI = 1.

La probabilidad de SVI = 1 con lcavol = 2.8269, lcp = 1.843 y lpsa = 3.285 es del 77.1%.


# 9. PCA-PCR
```{r}

datos.num
#pregunta 1
datos.num$svi <- as.numeric(datos$svi) # pasamos svi a numerico

variables_pca <- c("lcavol", "lweight", "lbph", "lcp", "svi") # cogemos las variables con las que haremos pca

datos_pca <- datos.num[,variables_pca ]  # datos con variables pca

datos_pca

pr.out <- prcomp(datos_pca, scale = TRUE)  # pca

biplot(pr.out, scale = 0)  #biplot

# el biplot muestra las coordenadas de cada variable en el espacio de las componentes principales, vemos que las variables que se encuentran juntas son las mas correlacionadas, como por ejemplo lcp y svi

summary(pr.out)

# en el summary se encuentran la desviacion estandar de cada componente, la proporcion de la varianza (desviacion estandar^2) de cada componente y suma acumulada de dicha varianza. Se observa que solamente el primer componente capta el 47.07% de la varianza.


corrplot::corrplot(cor(datos_pca), type = "upper", tl.cex = 0.5)


1.5341^2/(1.5341^2+1.1862^2+0.7258^2+0.66771^2+0.51651^2) # proportion of variance de PC1




# pregunta 2

pr.out

pr.out$sdev[1]  # desviacion estandar de pc1

# la desviacion estandar de pc1 es la maxima desviacion estandar que puede captarse con una combinacion lineal de las variables. las variables elegidas para combinarse linealmente han sido las mas correlacionadas: lcavol, lcp y svi, por lo que es esperable que la variabilidad (representada por la varianza) que representa la direccion de pc1, sea la mas alta de todas las combinaciones poisbles

# pregunta 3


summary(pr.out)

# observando la proporcion acumulada, se ve que con los tres primeros componentes principales ya se supera el 80% de la varianza total, ya que se tiene un 85.75%.


# ultima pregunta

x.pcr<-model.matrix(lpsa ~ .,datos.num)[,-1]  # datos de entrada
y.pcr<-datos.num$lpsa # datos de salida

y.pcr.test<-y.pcr[test] #datos de salida de test

pcr.fit<-pcr(lpsa ~ .,data=datos.num,subset=train,scale=TRUE,validation="CV") # regresion con componentes principales con conjunto de entrenamiento

summary(pcr.fit)

validationplot(pcr.fit,val.type="MSEP")  # vemos el error en funcion del numero de componentes principales,con n=5 el error es bajo


pcr.pred <- predict(pcr.fit, x.pcr[test, ], ncomp = 5) # predecimos con conjunto de test
pcr.MSE <- mean((pcr.pred- y.pcr.test)^2)  # comparamos resultado con datos de salida de test

pcr.MSE
ridge.MSE
lasso.MSE
rectaMul.MSE

# el MSE usando PCR es mayor que Lasso y Ridge, pero ligeramente menor que el de la regresión múltiple, por lo que si que es mejor que la del apartado 5, al menos con  n= 5 componentes principales


```

