---
title: "Práctica AE"
author: "Antonio Galián Gálvez"
date: "2024-10-28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(encoding = "UTF-8")
```

# 0. Cargamos los datos y eliminamos la columna train
```{r}
# Cargamos los datos con separador de tabulador
datos <- read.delim("prostate.data.txt", header = TRUE, sep = "\t")

# Eliminamos la columna train
datos <- datos[, -ncol(datos)]
```

# 1. Exploración de datos
```{r}
# Vemos las variables que hay
ncol(datos)

# Eliminamos la columna id
datos <- datos[, -1]

# Comprobamos si hay NA
sum(is.na(datos))

# Comprobamos si las variables estan estandarizadas
summary(datos)

dim(datos)
names(datos)
str(datos)
summary(datos)
```

- Hay 10 variables, 9 si quitamos el id del paciente
- Las variables son numéricas
- La variable correspondiente al identificador del paciente es la primera columna
- No hay valores nulos
- Las variables no están ni normalizadas ni estandarizadas
- Hay variables que están en escala logarítmica ya que algunas variables tienen valores negativos a pesar de estar definidas estrictamente positivas, como la concentración en ng/m

# 2

```{r}

attach(datos)

datos$svi <- as.factor(datos$svi)

datos$gleason <- as.factor(datos$gleason)

datos$age <- as.factor(datos$age)



str(datos)
```
# 3
```{r}
a <- datos[datos$gleason == "7", ]

b <- a[a$svi == "0", ]

num.a <- dim(a)[1]

num.b <- dim(b)[1]

porcentaje.ab <- num.b / (num.a) * 100

c <- datos[datos$svi == "0", ]

d <- c[c$gleason == "7", ]

num.c <- dim(c)[1]

num.d <- dim(d)[1]

porcentaje.cd <- num.d / (num.c) * 100


tabla <- table(svi, gleason)

addmargins(prop.table(tabla,1),2)*100
addmargins(prop.table(tabla,2),1)*100

```

# 4

```{r}

recta <- lm(lpsa ~ lcavol)

summary(recta)

plot(lcavol, lpsa, main='lpsa vs lcavol')
abline(recta, col = "red")

confint(recta, level = 0.95)

r1 <- residuals(recta)
sqrt(sum(r1^2) / (dim(datos)[1] - 2)) # RSE


```

# 5

```{r}

library(corrplot)

datos

num_cols <- which(sapply(datos, is.numeric))

corrplot(cor(datos[,num_cols]),type="upper",tl.cex=0.5)

datos.num <- datos[,c(-3,-7,-8)]

rectaMul <- lm(lpsa ~ .,data=datos.num)

summary(rectaMul)

```
#6

```{r}

library(glmnet)


x <- model.matrix(lpsa~.,datos.num)[,-1] #variables numericas

y <- datos.num$lpsa #variable de salida

set.seed(1)

train <- sample(1:nrow(x), nrow(x)/2) #filas de entrenamiento

test <- (-train)


y.test <- y[test]

malla <- 10^seq(10,-2,length=100) #mallado de lambdas

malla.ridge.train <- glmnet(x[train,],y[train],alpha=0,lambda=malla) # regresion ridge sin CV con conjunto de entrenamiento

plot(malla.ridge.train,xvar="lambda" ) # plot de betas vs log(lambda)



cv.out.train <- cv.glmnet(x[train,],y[train],alpha=0)  # regresion ridge con CV con conjunto de entrenamiento

bestlam.train<-cv.out.train$lambda.min  #lambda que minmiza el MSE

bestlam.train

ridge.train<-glmnet(x[train,],y[train],alpha=0, lambda=bestlam.train) #regresion ridge con CV con mejor lambda en conjunto de entrenamiento

coef(ridge.train)[,1]

rectaMul.train <- glmnet(x[train,],y[train],alpha=0, lambda=0)  # regresion multiple con datos de entrenamiento

coef(rectaMul.train)[,1]

#el intercept es mayor en ridge
# beta de lcp mayor en ridge
# los demas betas son menores en ridge

plot(cv.out.train)  #MSE vs log(lambda)

ridge.pred <- predict(ridge.train,newx=x[test,]) # predicion de 'ridge.train' en conjunto de testeo



mean((ridge.pred-y.test)^2) #MSE estimado de Ridge


rectaMul.pred <- predict(rectaMul.train,newx=x[test,])

mean((rectaMul.pred-y.test)^2) #MSE estimado de rectaMul

# El MSE de la regresion multiple de Ridge es menor que la de rectaMul

```

