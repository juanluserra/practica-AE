---
title: "Práctica Parte II. Aprendizaje Estadístico"
author: "Antonio Galián Gálvez, Juan Luis Serradilla Tormos"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Lo primero es importar las librerías necesarias y configurar el directorio de trabajo. También se seleccionará una misma semilla aleatoria para todo el cuaderno.
```{r}
# Seleccioamos el directorio actual como directorio de trabajo
# Si estamos compilando no lo hacemos para que no de error la compilación
if (interactive()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Cargamos las librerías necesarias
library(summarytools)
library(randomForest)
library(keras)
library(magrittr)

# Seleccionamos la semilla aleatoria para todo el cuaderno
set.seed(12345)
```

# 1. Agrupar variables.

**Enunciado:**
Agrupar todas las variables del conjunto por temática, describiendo lo que significa cada uno de los grupos de características que has identificado. Se pide básicamente, terminar el trabajo empezado arriba cuando se han descrito las características relativas al flujo, a los paquetes del flujo, y tiempo transcurrido entre paquete y paquete. Identifica y describe las ya identificadas arriba junto con las que faltan. Dentro de las características de cada conexión, para la mayoría de categorías de características, se distingue entre características forward y backward. Explicar en que se diferencian unas de otras y mencionar las referencias documentales que se han usado para responder. Desde el punto de vista del análisis de datos, ¿es relevante diferencia entre características forward y backward? ¿Por qué?

# 2. Estrategia de trabajo.

**Enunciado:**
Plantear y ejecutar una estrategia que te permita trabajar con el conjunto de datos de manera holgada si tener que usar necesariamente los 90K ejemplos en memoria.

Para poder trabajar con el datastet de manera holgada, es conveniente crear un subconjunto de datos que sea manejable. Para ello, se puede seleccionar un número de filas aleatorio que sea manejable y que permita realizar el análisis de manera eficiente. Este subconjunto de datos deberá tener la misma distribución de casos positivos y negativos que el dataset original para no desbalancear el modelo. Para conseguir esto basta con escoger una muestra aleatoria de, por ejemplo, un 10% de las filas del dataset original con una probabilidad uniforme y sin reemplazo.

Una forma de hacerlo puede ser con el código proporcionado:
```bash
awk 'BEGIN {srand()} NR==1 {print > "sampled.csv"; next} rand() <= 0.1 {print >> "sampled.csv"}' Keylogger_Detection.csv
```

Otra forma, aunque más lenta, sería cargar todo el dataset con R y seleccionar las filas aleatorias con la función `sample()`. A continuación se muestra un ejemplo de cómo hacerlo con R:
```{r, eval=FALSE}
# Cargamos el dataset
mydf <- read.csv("Keylogger_Detection.csv", header = TRUE, sep = ",")

# Seleccionamos un 10% de las filas
sampled <- mydf[sample(nrow(mydf), nrow(mydf) * 0.1), ]

# Guardamos el subconjunto de datos
write.csv(sampled, "sampled_2.csv", row.names = FALSE)
```

Antes de realizar el sampleo del dataset, es necesario preprocesar los datos. De esta forma, guardaremos los datos listos para trabajar. En este preprocesamiento, se eliminarán las columnas que no aportan información relevante, se eliminarán las filas con valores nulos, se eliminarán las filas duplicadas, se convertirán las variables a un formato adecuado, y se eliminarán las variables constantes. Una vez realizado este preprocesamiento, se procederá a realizar el sampleo del dataset.


## 2.1. Preprocesamiento del dataset
Cargamos los datos del dataset.
```{r}
# Cargamos los datos
mydf <- read.csv("Keylogger_Detection.csv", header = TRUE, sep = ",")
```

Vemos el tipo de variables que hay dentro del dataset para comprobar cuáles no aportan información relevante.
```{r}
# Realizamos un resumen del DataFrame
print(dfSummary(mydf), method = "render")

# Eliminamos las variables que no aportan información relevante
vars.to.remove <- c("X", "Flow.ID", "Timestamp", "Source.IP", "Destination.IP", "Source.Port", "Destination.Port")
mydf <- mydf[, !names(mydf) %in% vars.to.remove]
```

Se pueden ver algunas variables que no aportan información relevante, como "X", "Flow.ID", "Timestamp", "Source.IP", "Destination.IP", "Source.Port" y "Destination.Port".

- "X" es un índice que no aporta información relevante.
  
- "Flow.ID" es un identificador del flujo, por lo que no es una característica como tal.
  
- "Timestamp" es un instante temporal, no es una característica del flujo y por tanto no aporta información relevante.

- "Source.IP" y "Destination.IP" son identificadores de los dispositivos fuente y destino, respectivamente, y no son características del flujo. No aportan información al análisis. Además, no pueden interpretarse como variables ni numéricas ni categóricas (ya que tendrían demasiados valores únicos), y no se puede realizar un análisis siendo variables de tipo character.

- "Source.Port" y "Destination.Port" son los puertos de origen y destino, respectivamente, y no son características del flujo. No aportan información al análisis. No pueden interpretarse como variables categóricas (ya que tendrían demasiados valores únicos), y no dan información real de tipo numérico.


Una vez hemos eliminado las variables que no aportan información relevante, pasamos a eliminar las filas con valores nulos. Antes de nada, hay que comprobar que no vayamos a eliminar una gran cantidad de filas.
```{r}
# Buscamos las filas con valores nulos
na.coordinates <- which(is.na(mydf), arr.ind = TRUE)

# Vemos las distintas filas en las que se encuentran
na.rows <- unique(na.coordinates[, "row"])

# Calculamos la proporción respecto al número total de filas
na.prows <- length(na.rows) / nrow(mydf) * 100
paste("El porcentaje de filas con nulos es: ", na.prows, "%", sep = "")
```

Vemos que el porcentaje de filas a eliminar es muy bajo, así que eliminamos las filas con valores NA del DataFrame.
```{r}
# Eliminamos las filas con valores nulos
mydf <- na.omit(mydf)
```

Ahora, eliminamos las filas duplicadas.
```{r}
# Eliminamos las filas duplicadas
mydf <- mydf[!duplicated(mydf), ]
```

Vamos a eliminar las columnas con desviación estándar cero. Para ello, psaremos a numérico todas las variables que no sean claramente carácteres.
```{r}
# Pasamos a numérico las variables que les corresponde
mydf$Packet.Length.Std <- as.numeric(mydf$Packet.Length.Std)
```

Al analizar las variables vemos que hay una con una situación particular. Esta es "CWE.Flag.Count", que tiene valores "0" y "0.0". Además, los valores "0.0" no son pocos, suponen el 6% del total, más que los valores "1" de otros Flags como "FIN.Flag.Count", que solo el 1.7% son valores 1. Esto puede deberse a que por un error los valores 0.0 deberían ser 1. Si fuera un error de tipografía que pasará algunos valores 0 a 0.0 no habría tantos casos. Por lo tanto, se convertirán los valores "0.0" a "1" para que siga el formato común con los otros Flags.

```{r}
# Convertimos los "0.0" a 1
mydf$CWE.Flag.Count <- ifelse(mydf$CWE.Flag.Count == "0.0", "1", mydf$CWE.Flag.Count)

# Pasamos a numérico la variable
mydf$CWE.Flag.Count <- as.numeric(mydf$CWE.Flag.Count)

# Seleccionamos las columnas numéricas
num_cols <- which(sapply(mydf, is.numeric))

# Obtenemos las columnas con desviaciones estándar nulas
constant_cols <- which(sapply(mydf[, num_cols], sd) == 0)

# Eliminamos las columnas con valores constantes
mydf <- mydf[, -constant_cols]
```


Finalmente, volvemos a hacer una revisión de las variables del dataset y asignamos los tipos correspondientes a cada variable.
```{r}
# Pasamos los que tienen valores 0, 1 a factor.
mydf$Fwd.PSH.Flags <- as.factor(mydf$Fwd.PSH.Flags)
mydf$FIN.Flag.Count <- as.factor(mydf$FIN.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
mydf$PSH.Flag.Count <- as.factor(mydf$PSH.Flag.Count)
mydf$ACK.Flag.Count <- as.factor(mydf$ACK.Flag.Count)
mydf$URG.Flag.Count <- as.factor(mydf$URG.Flag.Count)
mydf$CWE.Flag.Count <- as.factor(mydf$CWE.Flag.Count)
mydf$Class <- as.factor(mydf$Class)
```


## 2.2. Sampleo del dataset
Ahora que tenemos el dataset preprocesado vamos a guardar el dataset en un fichero .csv para poder trabajar con él en el futuro. Además, crearemos un dataset con un 10% de las filas del dataset original para poder trabajar con él de manera holgada.
```{r}
# Guardamos el dataset preprocesado
write.csv(mydf, "preprocessed.csv", row.names = FALSE)

# Seleccionamos un 10% de las filas
sampled <- mydf[sample(nrow(mydf), nrow(mydf) * 0.1), ]

# Guardamos el subconjunto de datos
write.csv(sampled, "sampled_preprocesed.csv", row.names = FALSE)
```

Comprobamos que el dataset sampleado tiene la misma proporción de casos positivos y negativos que el dataset original.
```{r}
# Comprobamos la proporción de casos positivos y negativos en el dataset original
prop.table(table(mydf$Class)) * 100

# Comprobamos la proporción de casos positivos y negativos en el dataset sampleado
prop.table(table(sampled$Class)) * 100
```

Vemos que la proporción de casos positivos y negativos en el dataset sampleado es prácticamente la misma que en el dataset original. Por tanto, el dataset sampleado es válido para trabajar con él.
# 3. Elabora un análisis no supervisado (PCA) de las conexiones y explica lo que ves. ¿Cómo dirías que va a ser el problema de la elaboración de un modelo de clasificación que identifique conexiones de keystrokes en términos de dificultad? Razona la respuesta.

## 3.1. Lectura y preprocesado de los datos
Cuando cargamos el dataset preprocesado, hay que volver a transformar las variables numéricas a categóricas cuando corresponda.
```{r}
# Leemos el dataset preprocesado
mydf <- read.csv("sampled_preprocesed.csv", header = TRUE, sep = ",")

# Pasamos las variables correspondientes a factor
mydf$Protocol <- as.factor(mydf$Protocol)
mydf$Fwd.PSH.Flags <- as.factor(mydf$Fwd.PSH.Flags)
mydf$FIN.Flag.Count <- as.factor(mydf$FIN.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
mydf$PSH.Flag.Count <- as.factor(mydf$PSH.Flag.Count)
mydf$ACK.Flag.Count <- as.factor(mydf$ACK.Flag.Count)
mydf$URG.Flag.Count <- as.factor(mydf$URG.Flag.Count)
mydf$CWE.Flag.Count <- as.factor(mydf$CWE.Flag.Count)
mydf$Class <- as.factor(mydf$Class)

# Nos aseguramos de que la clase positiva sea "Keylogger" asignando como clase de referencia a "Benign"
mydf$Class <- relevel(mydf$Class, ref = "Benign")
```

## 3.2. Análisis PCA
Vamos a realizar el PCA con el conjunto preprocesado. Primero, seleccionamos las colunas numéricas.
```{r}
# Seleccionamos las columnas numéricas
mydf.numeric <- mydf[, sapply(mydf, is.numeric)]
```

Una vez tenemos las columnas numéricas seleccionadas, realizamos el PCA. No hace falta estandarizar las variales previamente ya que el PCA lo hace por nosotros con el parámetro `r scale = TRUE`.
```{r}
# Realizamos PCA
pr.out <- prcomp(mydf.numeric, scale = TRUE)
```

Vamos a realizar un análisis del PCA para ver qué información nos aporta. Para ello, primero vamos a ver la varianza acumulada, para determinar cuáles son las componentes principales más importantes.
```{r}
# Vemos la varianza explicada
summary(pr.out)

# Realizamos un gráfico de la varianza acumulada
pr.out.cumvar <- cumsum(pr.out$sdev^2 / sum(pr.out$sdev^2))
plot(
  pr.out.cumvar,
  xlab = "Principal Component",
  ylab = "Cumulative Proportion of Variance Explained",
  type = "b"
)

# Seleccionamos las componentes principales que expliquen al menos el 80% de la varianza
ncomp <- which(pr.out.cumvar > 0.8)[1]
ncomp
```

Vemos que necesitamos las 10 primeras componentes principales para explicar el 80% de la varianza. Solo con las 2 primeras componentes principales explicamos tan solo el 37.5% de la varianza, una cifra muy baja. Por tanto, necesitamos muchas componentes principales para explicar la varianza de los datos.

Vamos a realizar un biplot para ver cómo se relacionan las variables con las dos primeras componentes principales.
```{r}
# Realizamos un biplot solo con los vectores, no con los datos
png("biplot.png")
biplot(pr.out, scale = 0, xlim = c(-15, 15), ylim = c(-15, 15))
dev.off()
```

Como el dataset tiene demasiadas varaibles, el biplot se guarda en un .png para poder consultarse cuando se desee. Se puede ver en esta imagen que tenemos demasiados predictores y demasiados valores, no se puede sacar nada en claro. Además, como PC1 y PC2 explican solo el 37.5% de la varianza el biplot tampoco sería representativo.

Por todo esto, podemos ver que la elaboración de un modelo de clasificación que identifique conexiones de keystrokes va a ser un problema complicado. Tenemos muchos predictores y ver las relaciones entre ellos no va a ser una tarea sencilla. Si intentásemos reducir la dimensionalidad del problea con PCA como mucho podríamos reducir a 10 dimensiones, lo cual sigue siendo una gan cantidad de variables.


# 4. ¿Es posible identificar si la conexión es benigna o maligna mediante un modelo de regresión? ¿Qué tipo de modelo de regresión habría que usar en este caso? ¿Cuáles son las características relevantes? ¿Cómo se comporta el modelo?

Para poder identificar si la conexión es benigna o maligna mediante un modelo de regresión, usaremos un modelo de regresión logística. Realizaremos unas predicciones con el resultado del modelo y veremos cómo funciona para el dataset. No usaremos conjunto de prueba ni de test, ya que simplemente queremos ver cómo de eficaz es con nuestro conjunto de datos.

```{r}
# Creamos el modelo de regresión logística
logr <- glm(Class ~ ., data = mydf, family = binomial)

# Vemos un resumen de logr
summary(logr)

# Vemos los coeficientes que han generado aliasing en el modelo
alias.info <- alias(logr)
alias.matrix <- as.matrix(alias.info$Complete)
alias.matrix <- abs(alias.matrix[, apply(alias.matrix, 2, sd) != 0])
alias.matrix <- round(alias.matrix)
alias.matrix
```

Vemos que hay variables que han generado aliasing en el modelo, lo que significa que están perfectamente correlacionadas con otras variables (variables de filas con variables de columnas). El modelo de regesión logística directamente no tendrá en cuenta estas variables. 

Una vez tenemos el modelo creado, realizamos unas preddicciones. Con estas predicciones se harán unas tablas de confusión para ver cómo se comporta el modelo.
```{r}
# Realizamos las predicciones
logr.probs <- predict(logr, type = "response")
logr.pred <- ifelse(logr.probs > 0.5, "Benign", "Keylogger")
logr.table <- table(logr.pred, mydf$Class)
dimnames(logr.table) <- list(
  Predict = c("Benign", "Keylogger"),
  Real = c("Benign", "Keylogger")
)

# Realizamos las tablas de confusión
addmargins(round(prop.table(logr.table) * 100, 2))

# Hacemos una suma por columnas para ver la precisión
addmargins(round(prop.table(logr.table, 2) * 100, 1), 1)

# Hacemos una suma por filas para ver la sensibilidad
addmargins(round(prop.table(logr.table, 1) * 100, 1), 2)
```

Analizando la tabla de confusión vemos que el modelo no es capaz de predecir correctamente si una conexión es benigna o maligna.
- De las conexiones que son benignas, el modelo solo es capaz de predecir correctamente el 2.9%. 
- De las conexiones que son keylogger, el modelo predice correctamente el 78.7%.
- De las conexiones que se han predicho como benignas, solo el 16.2% eran realmente benignas, dejando una tasa de falso positivo del 83.8%.
- De las conexiones que se han predicho como keylogger, solo el 36.4% eral realmente keylogger, dejando una tasa de falso negativo del 63.6%.
  
Con este análisis podemos concluir que el modelo es nefasto prediciendo conexiones benignas. Además, aunque prediga la mayoría de las conexiones keylogger, tiene una gran tasa de falso positivo, por lo que no es un modelo fiable. Por tanto, no es posible identificar si la conexión es benigna o maligna mediante un modelo de regresión logística.


# 5. Predicción con modelos de machine learning

**- Explicar brevemente el tipo de modelo que genera el algoritmo, y cuál es la estrategia de dicho algoritmos para construir el modelo.**

**- Indicar si el algoritmo en cuestión tiene algún requisito en cuanto a si se han de preprocesar los datos (e.g. escalado, imputación de valores nulos, etc.) y cómo.**

**- Identificar y explicar cada uno de sus hiperparámetros**

**- Detallar una estrategia para la generación del grid de valores para hiperparámetros a usar.**

**- Ejecutar la estrategia, generar los modelos y seleccionar el mejor siguiendo la estrategia explicada arriba**

**- En primer lugar dividimos los datos en un conjunto de train y otro de test. Para el conjunto de test se cogerán 10000 datos, el resto será de train.**


## 5.1. Bagging
```{r}
# Definimos el conjunto de test
test_indices <- sample(1:nrow(mydf), 10000)
test <- mydf[test_indices, ]

# Definimos el conjunto de train
train_indices <- setdiff(1:nrow(mydf), test_indices)
train <- mydf[train_indices, ]
```

Para llevar a cabo el proceso de validación cruzada, dividiremos el conjunto de train en n pliegues. El número elegido será n=5 pliegues.

```{r}
# Definimos el número de pliegues
n <- 5

# Generamos un vector de igual longitud que el numero de filas de train, asociando cada valor a un pliegue
folds_indexes <- sample(rep(1:n, length.out = nrow(train)))

# Obtenemos los pliegues aplicando la secuencia anterior a los datos de train
folds <- split(train, folds_indexes)
```

Ahora que ya tenemos los pliegues creados, podemos empezar a usar los algoritmos de machine learning para predecir la variable Class. Los algortimos escogidos serán dos: Bagging y Red neuronal.

# Bagging

En este caso los hiperparámetros entre los cuales podemos elegir son dos: el número de variables consideradas en cada split de cada árbol (mtry) y el número de árboles que se crearán (ntree). Crearemos distintos vectores que contendrán dos componentes, siendo cada una un valor posible para su hiperparámetro correspondiente.

Para determinar el rango de valores que podemos probar para mtry, podemos partir del valor usado por defecto en el algoritmo Random Forest para este hiperparámetro, sqrt(p), donde p es el número de predictores.

```{r}
# Vemos cuál es el valor que usa el algoritmo Random Forest por defecto, sqrt(p)
m_rf <- sqrt(dim(train)[2])
```

Podemos tomar como referencia mtry=8.

Lo que haremos será crear seis vectores
```{r}
length(folds)
```

```{r}
v1 <- c(6, 100)

v2 <- c(8, 150)

v3 <- c(10, 200)


v <- rbind(v1, v2, v3)



test_fold <- rep(0, length(folds))

mean_accuracy <- rep(0, dim(v)[1])




for (i in 1:dim(v)[1]) {
  for (k in seq(1:length(folds))) {
    # Juntamos todos los folds menos el k
    data <- do.call(rbind, folds[-k])

    # Creamos el modelo de bagging
    bag <- randomForest(Class ~ .,
      data = data,
      mtry = v[i, 1], ntree = v[i, 2], importance = TRUE
    )

    # Predecimos con el pliegue que hemos dejado fuera del entrenamiento
    yhat.bag <- predict(bag, newdata = folds[[k]])

    # Guardamos la accuracy obtenida comparando las predicciones y los outputs reales del pliegue k
    test_fold[k] <- MLmetrics::Accuracy(yhat.bag, folds[[k]]$Class)
  }

  # Hacemos la media de los accuracy de todos los pliegues
  mean_accuracy[i] <- mean(test_fold)
}
```


```{r}
for (i in v) {
  print(i)
}
```

```{r}
mean_accuracy
# Guardamos mean_accuracy en un archivo .csv
results <- data.frame()
results$mean_accuracy <- mean_accuracy
results$mtry <- v[, 1]
results$ntree <- v[, 2]

write.csv(results, "results_bagging.csv", row.names = FALSE)
```


## 5.2. Red neuronal
Vayamos ahora con el segundo algortimo. Ahora los hiperparámetros son el número de nodos de la capa oculta y el porcentaje de dropout para la regularización.

```{r}
v1 <- c(50, 0.2)

v2 <- c(100, 0.5)

v3 <- c(200, 0.8)

v <- rbind(v1, v2, v3)

test_fold <- rep(0, length(folds))

mean_accuracy <- rep(0, dim(v)[1])



for (i in 1:dim(v)[1]) {
  for (k in seq(1:length(folds))) {
    # Juntamos todos los folds menos el k
    data <- do.call(rbind, folds[-k])

    modnn <- keras_model_sequential() %>%
      layer_dense(units = v[i, 1], activation = "sigmoid", input_shape = ncol(data)) %>%
      layer_dropout(rate = v[i, 2]) %>%
      layer_dense(units = 2, activation = "softmax")


    # Predecimos con el pliegue que hemos dejado fuera del entrenamiento
    yhat.modnn <- predict(modnn, newdata = folds[[k]])

    # Guardamos la accuracy obtenida comparando las predicciones y los outputs reales del pliegue k
    test_fold[k] <- MLmetrics::Accuracy(yhat.modnn, folds[[k]]$Class)
  }

  # Hacemos la media de los accuracy de todos los pliegues
  mean_accuracy[i] <- mean(test_fold)
}
```

