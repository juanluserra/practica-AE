---
title: "Práctica Parte II. Aprendizaje Estadístico"
author: "Antonio Galián Gálvez, Juan Luis Serradilla Tormos"
date: "2024-12-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Seleccioamos el directorio actual como directorio de trabajo
# Si estamos compilando no lo hacemos para que no de error la compilación
if (interactive()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Cargamos las librerías necesarias
library(summarytools)
library(randomForest)
library(keras)

# Seleccionamos la semilla aleatoria para todo el cuaderno
set.seed(12345)
```

Una vez se han importado las librerías, lo primero que debemos realizar es la lectura de los datos. En este caso partimos del ejemplo reducido 'sampled.csv'.
```{r}
# Cargamos los datos
mydf <- read.csv("sampled.csv", header = TRUE, sep = ",")

```

Dado que al leerse el fichero .csv es generada una columna de nombre "X" a modo de índice, la borramos porque resulta redundante.´

```{r}
# Eliminamos la columna X, que es la columna de los índices
mydf <- mydf[, names(mydf) != "X"]
```



El dataset es muy grande, tanto en el número de medidas como en el de predictores. Lo más conveniente es realizar un preprocesamiento de los datos para quedarnos solamente con la información relevante. Este filtrado constará de varios pasos, ya que existen distintos aspectos que pueden depurarse. Los dos primeros serán relativos a las filas, y los dos siguientes a las columnas. Empezamos por el tratamiento de las filas.

El primero paso respecto el depuramiento de las filas será eliminar aquellas donde existan valores nulos, es decir, de tipo NA. Para esto debemos averiguar primero cúantos existen en el dataset y la proporción que suponen respecto al conjunto total de datos.
```{r}
# Vemos la cantidad de datos nulos presentes en los datos
na.number <- sum(is.na(mydf))
na.prop <- na.number / nrow(mydf) / ncol(mydf) * 100
paste("La proporcion de nulos es: ", na.prop, "%", sep = "")
```
El número total de valores nulos son 117, y son un 0.0026% de la cantidad total de datos, lo que resulta muy favorable al ser tan pequeño. Ahora bien, dado que la manera de proceder será eliminar sus filas debemos averiguar en cuáles de ellas se encuentran. Además, es importante saber a qué valor de la columna Class corresponden estas filas, para no desbalancear la proporción de las categorías de Class presente en el dataset. Vemos en primer lugar cuál es esta proporción originalmente, incluyendo todos los datos.

```{r}
# Vemos la proporción de clases en la columna a predecir
proptable.old <- round(prop.table(table(mydf$Class))*100,2)
proptable.old
```
Un 59.14% de filas son de tipo 'Benign' y un 40.86% son de tipo "Keylogger". La proporción es desigual pero no está excesivamente desbalanceada.

Vemos ahora las filas distntas en las que se encuentran los valores nulos.

```{r}
# Primero vemos las coordenadas de los valores nulos
na.coordinates <- which(is.na(mydf), arr.ind = TRUE)

# Vemos las distintas filas en las que se encuentran
na.rows <- unique(na.coordinates[, "row"])
na.rows

# Calculamos la proporción respecto al número total de filas
na.prows <- length(na.rows) / nrow(mydf) * 100
paste("El porcentaje de filas con nulos es: ", na.prows, "%", sep = "")

```
Resulta que todos los NA están concentrados en 3 filas, las cuales suponen un 0.0057% respecto al número total de filas, lo que implica que no se perderá apenas información. Comprobamos ahora la clase de estas 3 filas.

```{r}

# De las tres filas, vemos cuáles son de la clase "Benign" y cuales de "Keylogger"
mydf[na.rows, ]$Class

```
Las 3 filas son de la misma clase, "Keylogger", pero igualmente pueden eliminarse ya que no afectarán mucho a la proporción entre clases. Eliminamos las filas y comprobamos que es así.

```{r}
# Todas son de Keylogger, pero al ser solo 3, podemos eliminar dichas filas.
mydf <- na.omit(mydf)

# Comprobamos de nuevo la proporción de la aparición de ambos
proptable.new <- round(prop.table(table(mydf$Class))*100,2)
proptable.old
proptable.new
```
La diferencias que existen en las proporciones son de un 0.01% mayor en la clase "Benign" y un 0.01% menor en la clase "Keylogger", por lo que es completamente justificable la acción que hemos tomado.

La segunda acción que tomamos respecto a las filas es averiguar cuáles se encuentran repetidas, y acto seguido, eliminarlas. De esta manera podemos liberarnos de información redundante.

```{r}
# Comprobamos si existen filas duplicadas
dup.nrows <- sum(duplicated(mydf))
dup.prows <- round(dup.nrows / nrow(mydf) * 100,2)
dup.nrows
paste("El porcentaje de filas duplicadas es: ", dup.prows, "%", sep = "")
```
Tenemos 7089 filas duplicadas, un 13.49% del total. Las eliminamos.

```{r}
# Eliminamos las filas duplicadas
mydf <- mydf[!duplicated(mydf), ]
```

Pasamos ahora al preprocesamiento de las columnas. El primer paso es echar un vistazo general a sus características.

```{r}
print(dfSummary(mydf),method="render")
```

Existen variables que por su naturaleza en este contexto sabemos que no serán útiles para nuestra predicción, así que es conveniente prescindir de ellas.

-Flow.ID. Es un identificador del flujo, por lo que no es una característica como tal.

-Timestamp. Al tratarse de un instante temporal, no es cualidad intínseca del flujo.

-Source.IP. Esto es un identificador del dispositivo fuente, lo cual no caracteriza el flujo en sí.

-Destination.IP. Análogamente a la variable anterior, es un identificador del dispositivo de destino, y no es una característica del flujo.

-Source.Port. Esta variable identifica el puerto de la fuente, lo que no determina cómo es el flujo en sí.

-Destination.Port. De forma análoga a la variable anterior, esto identifica el puerto del destino, por lo que no caracteriza el flujo.

(Falta refinar todo lo anterior para justificar cómo realmente no influyen en las cualidades del flujo)

Las eliminamos por tanto.

```{r}

# Eliminamos las variables que no tengan relevancia en nuestro contexto
vars.to.remove <- c("Flow.ID", "Timestamp", "Source.IP", "Destination.IP", "Source.Port", "Destination.Port")
mydf <- mydf[, !names(mydf) %in% vars.to.remove]

```

Una vez hemos eliminado estas variables pasamos a comprobar si todas las restantes están en el formato adecuado ('numeric','factor', etc.). Este paso es necesario para el filtrado de variables constantes que realizaremos más adelante.

```{r}
str(mydf)
```


La primera variable que debemos pasar a un formato adecuado es la de salida, "Class", ya que originalmente aparece como "character". Será necesario convertirla a "factor" dado que se trata de categorías.

```{r}
# Pasamos Class a factor
mydf$Class <- as.factor(mydf$Class)
```

Algunas variables de entrada numéricas presentan un rango de entre 0 y 1, lo que induce a pensar que realmente se tratan de variables binarias, por lo que deben ser convertidas a factor.

```{r}
# Pasamos los que son de rango 0-1 a factor
mydf$Fwd.PSH.Flags <- as.factor(mydf$Fwd.PSH.Flags)
mydf$FIN.Flag.Count <- as.factor(mydf$FIN.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
mydf$PSH.Flag.Count <- as.factor(mydf$PSH.Flag.Count)
mydf$ACK.Flag.Count <- as.factor(mydf$ACK.Flag.Count)
mydf$URG.Flag.Count <- as.factor(mydf$URG.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
```

De las variables numéricas restantes existe una que, dada su naturaleza, encaja más como factor que como número. Se trata de "Protocol", y, como su nombre indica, hace referencia al protocolo usado en el flujo. Al existir distintos tipos de protocolos, puede entenderse esta variable como una variable categórica. La pasamos por tanto a factor.

```{r}

# Cambiamos "Protocol" a factor
mydf$Protocol <- as.factor(mydf$Protocol)

```

Existe una variable definida como "character" llamada "CWE.Flag.Count", cuyos posibles valores son "0" y "0.0". Dado que forma parte del grupo de variables categóricas que contienen "Flag.Count", es bastante probable que "CWE.Flag.Count" también sea de este tipo y que haya existido un error en la codificación de los valores binarios 0 y 1. Lo solucionaremos conviertiendo "0.0" a 1, de manera que pueda seguir el formato común.

```{r}

#Convertimos los "0.0" a 1
mydf$CWE.Flag.Count <- ifelse(mydf$CWE.Flag.Count == "0.0", "1",mydf$CWE.Flag.Count)

#Pasamos la variable a factor
mydf$CWE.Flag.Count <- as.factor(mydf$CWE.Flag.Count)

```

Ahora que todas las variables de tipo "numeric" son realmente variables numéricas, lo último que hay que hacer es eliminar las que tienen desviación estándar nula.


```{r}

# Seleccionamos las columnas numéricas
num_cols <- which(sapply(mydf, is.numeric))

# Obtenemos las dcolumnas con desviaciones estandar nulas
constant_cols <- which(sapply(mydf[, num_cols], sd) == 0)
constant_cols <- num_cols[constant_cols]

```


Existen un total de 11 columnas númericas constantes. Las eliminamos.

```{r}

# Eliminamos las columnas con valores constantes
mydf <- mydf[, -constant_cols]

```


Con esto ya hemos terminado el preprocesamiento. Podemos observar de qué tamaño es nuestro dataset preprocesado y listo para el análisis.

```{r}

dim(mydf)
```




# Apartado PCA







#```{r}
# Realizamos PCA

# Seleccionamos las columnas numéricas
mydf.numeric <- mydf[, sapply(mydf, is.numeric)]

# Estandarizamos los datos
mydf.numeric <- scale(mydf.numeric)

# Realizamos PCA
pr.out <- prcomp(mydf.numeric, scale = TRUE)

pr.out.modified <- pr.out
pr.out.modified$x <- pr.out$x[1:1000, ]
biplot(pr.out.modified, scale = 0)
# Observando los primeros PC pueden extraerse relaciones entre las variables con valores más altos positiva o negativamente.

# En el PC1 se tiene que las variables más relevantes son:
#-Bwd.Packet.Length.Max, 0.230922856
#-Bwd.Packet.Length.Std, 0.217046556
#-Fwd.Packet.Length.Max, 0.216276331
#-Bwd.Packet.Length.Mean, 0.207408658

# Lo que indica que esta componente está relacionada con el tamaño de los paquetes tanto backward como forward, y las anteriores variables están relacionadas positivamente


# En el PC2 se resaltan las siguientes variables:
#-Bwd.Packet.Length.Mean, 0.155033239
#-Bwd.Packet.Length.Max, 0.131454508
#-Flow.IAT.Mean, -0.229527468
#-Flow.Duration, -0.241615961

# Esta componente relaciona la longitud de los paquetes backward y parámetros del flujo. Cuanto mayor es la longitud de los paquetes, menor es la media de IAT y la duración del flujo.

# Vemos la varianza explicada
summary(pr.out)

# Calculamos las varianzas de cada componente principal
pc.var <- pr.out$sdev^2

# Ahora podemos calcular el PVE = Proportion of Variance Explained
pve <- pc.var / sum(pc.var)

# Con esto graficamos cómo varían el PVE y el PVE acumulado en función del número de componentes principales
par(mfrow = c(1, 2))
plot(pve,
  xlab = "Principal Component",
  ylab = "Proportion of Variance Explained", ylim = c(0, 1),
  type = "b"
)
plot(cumsum(pve),
  xlab = "Principal Component",
  ylab = "Cumulative Proportion of Variance Explained",
  ylim = c(0, 1), type = "b"
)
par(mfrow = c(1, 1))
# Seleccionamos las componentes principales que expliquen al menos el 80% de la varianza
ncomp <- which(cumsum(pve) > .8)[1]
ncomp

# Seleccionamos una cantidad de filas del dataset más pequeá para poder visualizar los datos
pr.out.modified <- pr.out
pr.out.modified$x <- pr.out$x[1:1000, ]
par(mfrow = c(1, 1))
biplot(pr.out.modified, scale = 0)

# Comprobamos las variables más importantes en cada PCI
PCI <- 1
components <- pr.out$rotation[, PCI]**2
components <- sort(components, decreasing = TRUE)
cumsum(components)
length(components[cumsum(components) <= 0.8])

# Hacemos lo contrario, dada una variable vemos como se relaciona con los PCIs
components.names <- c()
threshold <- 0.8
for (var in 1:ncol(mydf.numeric)) {
  components <- pr.out$rotation[var, ]**2
  components <- sort(components, decreasing = TRUE)
  components.valids <- which(cumsum(components) <= threshold)
  components.names <- c(components.names, names(components.valids))
}
components.names <- unique(components.names)
length(components.names)


var <- 3
components <- pr.out$rotation[var, ]**2
components <- sort(components, decreasing = TRUE)
cumsum(components)
length(components[cumsum(components) <= 0.8])

#-------------------------------------------
# A partir de aquí es solo para experimentar
#-------------------------------------------

# Hacemos un producto escalar entre las variables para determinar las que están más relacionadas
#```






#```{r}
# 4

# Este problema sí podría resolverse con un algoritmo de regresión

# Dado que estamos antes un problema de clasificación binaria, el método de regresión que podemos usar es la regresión logística, especialmente una regresión logística múltiple al tener más de un predictor.

# Vamos a crear un modelo de este tipo de regresión.
mydf.lr <- mydf[,sapply(mydf,is.numeric)]

mydf.lr$Class <- mydf$Class

str(mydf)

lr <- glm(Class ~ ., data = mydf.lr,family=binomial)

summary(lr)

lr.probs<-predict(lr,type="response")


# Creamos un dataset con variables numéricas y factores
logr <- glm(Class ~ ., data = mydf, family = binomial)
summary(logr)

logr.probs <- predict(logr, type = "response")
logr.pred <- ifelse(logr.probs > 0.5, "Benigno", "Maligno")
logr.table <- table(logr.pred, mydf$Class)
addmargins(round(prop.table(logr.table) * 100, 2))
addmargins(round(prop.table(logr.table, 2) * 100, 1), 1)
addmargins(round(prop.table(logr.table, 1) * 100, 1), 2)
#```


# Predicción con modelos de machine learning

En primer lugar dividimos los datos en un conjunto de train y otro de test. Para el conjunto de test se cogerán 10000 datos, el resto será de train.

```{r}

#Definimos el conjunto de test
test_indices <- sample(1:nrow(mydf), 10000)
test <- mydf[test_indices, ]

#Definimos el conjunto de train
train_indices <- setdiff(1:nrow(mydf), test_indices)
train <- mydf[train_indices, ]
```

Para llevar a cabo el proceso de validación cruzada, dividiremos el conjunto de train en n pliegues. El número elegido será n=5 pliegues.

```{r}

#Definimos el número de pliegues
n <- 5

# Generamos un vector de igual longitud que el numero de filas de train, asociando cada valor a un pliegue
folds_indexes <- sample(rep(1:n, length.out = nrow(train)))

# Obtenemos los pliegues aplicando la secuencia anterior a los datos de train
folds <- split(train, folds_indexes)

```

Ahora que ya tenemos los pliegues creados, podemos empezar a usar los algoritmos de machine learning para predecir la variable Class. Los algortimos escogidos serán dos: Bagging y Red neuronal.

# Bagging

En este caso los hiperparámetros entre los cuales podemos elegir son dos: el número de variables consideradas en cada split de cada árbol (mtry) y el número de árboles que se crearán (ntree). Crearemos distintos vectores que contendrán dos componentes, siendo cada una un valor posible para su hiperparámetro correspondiente.

Para determinar el rango de valores que podemos probar para mtry, podemos partir del valor usado por defecto en el algoritmo Random Forest para este hiperparámetro, sqrt(p), donde p es el número de predictores.

```{r}

#Vemos cuál es el valor que usa el algoritmo Random Forest por defecto, sqrt(p)
m_rf <- sqrt(dim(train)[2])
```

Podemos tomar como referencia mtry=8.

Lo que haremos será crear seis vectores
```{r}
length(folds)
```

```{r}
v1 <- c(6,100)

v2 <- c(8,150)

v3 <- c(10,200)


v <- rbind(v1,v2,v3)



test_fold <- rep(0,length(folds))

mean_accuracy <- rep(0,dim(v)[1])




for (i in 1:dim(v)[1]) {
  for (k in seq(1:length(folds))) {
          
          #Juntamos todos los folds menos el k
          data <- do.call(rbind, folds[-k])
    
          #Creamos el modelo de bagging
          bag <- randomForest(Class ~ ., data = data,
                           mtry = v[i,1], ntree = v[i,2], importance = TRUE)
          
          #Predecimos con el pliegue que hemos dejado fuera del entrenamiento
          yhat.bag <- predict(bag,newdata = folds[[k]])
          
          #Guardamos la accuracy obtenida comparando las predicciones y los outputs reales del pliegue k
          test_fold[k] <- MLmetrics::Accuracy(yhat.bag, folds[[k]]$Class)
  }
  
    #Hacemos la media de los accuracy de todos los pliegues
    mean_accuracy[i] <-  mean(test_fold)
}
```


```{r}
for (i in v) {
  print(i)
}
```

```{r}

mean_accuracy
```


#Vayamos ahora con el segundo algortimo. Ahora los hiperparámetros son el número de nodos de la capa oculta y el porcentaje de dropout para la regularización.


```{r}

v1 <- c(50,0.2)

v2 <- c(100,0.5)

v3 <- c(200,0.8)

v <- rbind(v1,v2,v3)

test_fold <- rep(0,length(folds))

mean_accuracy <- rep(0,dim(v)[1])



for (i in 1:dim(v)[1]) {
  for (k in seq(1:length(folds))) {
          
          #Juntamos todos los folds menos el k
          data <- do.call(rbind, folds[-k])
    
          modnn <- keras_model_sequential() %>%
                   layer_dense(units = v[i,1], activation = "sigmoid", input_shape = ncol(data)) %>%
                   layer_dropout(rate = v[i,2]) %>% layer_dense(units = 2, activation = "softmax")
          
          
          #Predecimos con el pliegue que hemos dejado fuera del entrenamiento
          yhat.modnn <- predict(modnn,newdata = folds[[k]])
          
          #Guardamos la accuracy obtenida comparando las predicciones y los outputs reales del pliegue k
          test_fold[k] <- MLmetrics::Accuracy(yhat.modnn, folds[[k]]$Class)
  }
  
    #Hacemos la media de los accuracy de todos los pliegues
    mean_accuracy[i] <-  mean(test_fold)
}

```

