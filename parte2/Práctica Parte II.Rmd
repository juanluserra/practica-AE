---
title: "Práctica 2"
author: "Antonio Galián Gálvez, Juan Luis Serradilla Tormos"
date: "2024-12-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Seleccioamos el directorio actual como directorio de trabajo
# Si estamos compilando no lo hacemos para que no de error la compilación
if (interactive()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Cargamos las librerías necesarias
library(summarytools)

# Seleccionamos la semilla aleatoria para todo el cuaderno
set.seed(12345)
```


```{r}
# Cargamos los datos
mydf <- read.csv("sampled.csv", header = TRUE, sep = ",")

# Eliminamos la columna X, que es la columna de los índices
mydf <- mydf[, names(mydf) != "X"]

# Vemos la proporción de clases en la columna a predecir
proptable.old <- prop.table(table(mydf$Class))

# Vemos la cantidad de datos nulos presentes en los datos
na.number <- sum(is.na(mydf))
na.prop <- na.number / nrow(mydf) / ncol(mydf) * 100
paste("La proporcion de nulos es: ", na.prop, "%", sep = "")

# Primero vemos las coordenadas de los valores nulos
na.coordinates <- which(is.na(mydf), arr.ind = TRUE)

# Vemos las distintas filas en las que se encuentran
na.rows <- unique(na.coordinates[, "row"])
na.prows <- length(na.rows) / nrow(mydf) * 100
paste("El porcentaje de filas con nulos es: ", na.prows, "%", sep = "")

# De las tres filas, vemos cuáles son de la clase "Benign" y cuales de "Keylogger"
mydf[na.rows, ]$Class

# Todas son de Keylogger, pero al ser solo 3, podemos eliminar dichas filas.
mydf <- na.omit(mydf)

# Comprobamos de nuevo la proporción de la aparición de ambos
proptable.new <- prop.table(table(mydf$Class))
proptable.old
proptable.new
# Es prácticamente igual que anteriormente

# Comprobamos si existen filas duplicadas
dup.nrows <- sum(duplicated(mydf))
dup.prows <- dup.nrows / nrow(mydf) * 100
paste("El porcentaje de filas duplicadas es: ", dup.prows, "%", sep = "")

# Las eliminamos
mydf <- mydf[!duplicated(mydf), ]

# Pasamos Class a factor
mydf$Class <- as.factor(mydf$Class)

# Eliminamos la variable timestamp
mydf <- mydf[, names(mydf) != "Timestamp"]

# Cambiamos el tipo de las variables que sea necesario
mydf$Class <- as.factor(mydf$Class)
mydf$Protocol <- as.factor(mydf$Protocol)
mydf$Destination.Port <- as.character(mydf$Destination.Port)
mydf$Source.Port <- as.character(mydf$Source.Port)
mydf$CWE.Flag.Count <- as.numeric(mydf$CWE.Flag.Count)

# Pasamos los que son de rango 0-1 a factor
mydf$Fwd.PSH.Flags <- as.factor(mydf$Fwd.PSH.Flags)
mydf$FIN.Flag.Count <- as.factor(mydf$FIN.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
mydf$PSH.Flag.Count <- as.factor(mydf$PSH.Flag.Count)
mydf$ACK.Flag.Count <- as.factor(mydf$ACK.Flag.Count)
mydf$URG.Flag.Count <- as.factor(mydf$URG.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)

# Seleccionamos las columnas numéricas
num_cols <- which(sapply(mydf, is.numeric))

# Obtenemos las dcolumnas con desviaciones estandar nulas
constant_cols <- which(sapply(mydf[, num_cols], sd) == 0)
constant_cols <- num_cols[constant_cols]

# Eliminamos las columnas con valores constantes
mydf <- mydf[, -constant_cols]
```


```{r}
# Realizamos PCA

# Seleccionamos las columnas numéricas
mydf.numeric <- mydf[, sapply(mydf, is.numeric)]

# Estandarizamos los datos
mydf.numeric <- scale(mydf.numeric)

# Realizamos PCA
pr.out <- prcomp(mydf.numeric, scale = TRUE)

pr.out.modified <- pr.out
pr.out.modified$x <- pr.out$x[1:1000, ]
biplot(pr.out.modified, scale = 0)
# Observando los primeros PC pueden extraerse relaciones entre las variables con valores más altos positiva o negativamente.

# En el PC1 se tiene que las variables más relevantes son:
#-Bwd.Packet.Length.Max, 0.230922856
#-Bwd.Packet.Length.Std, 0.217046556
#-Fwd.Packet.Length.Max, 0.216276331
#-Bwd.Packet.Length.Mean, 0.207408658

# Lo que indica que esta componente está relacionada con el tamaño de los paquetes tanto backward como forward, y las anteriores variables están relacionadas positivamente


# En el PC2 se resaltan las siguientes variables:
#-Bwd.Packet.Length.Mean, 0.155033239
#-Bwd.Packet.Length.Max, 0.131454508
#-Flow.IAT.Mean, -0.229527468
#-Flow.Duration, -0.241615961

# Esta componente relaciona la longitud de los paquetes backward y parámetros del flujo. Cuanto mayor es la longitud de los paquetes, menor es la media de IAT y la duración del flujo.

# Vemos la varianza explicada
summary(pr.out)

# Calculamos las varianzas de cada componente principal
pc.var <- pr.out$sdev^2

# Ahora podemos calcular el PVE = Proportion of Variance Explained
pve <- pc.var / sum(pc.var)

# Con esto graficamos cómo varían el PVE y el PVE acumulado en función del número de componentes principales
par(mfrow = c(1, 2))
plot(pve,
  xlab = "Principal Component",
  ylab = "Proportion of Variance Explained", ylim = c(0, 1),
  type = "b"
)
plot(cumsum(pve),
  xlab = "Principal Component",
  ylab = "Cumulative Proportion of Variance Explained",
  ylim = c(0, 1), type = "b"
)
par(mfrow = c(1, 1))
# Seleccionamos las componentes principales que expliquen al menos el 80% de la varianza
ncomp <- which(cumsum(pve) > .8)[1]
ncomp

# Seleccionamos una cantidad de filas del dataset más pequeá para poder visualizar los datos
pr.out.modified <- pr.out
pr.out.modified$x <- pr.out$x[1:1000, ]
par(mfrow = c(1, 1))
biplot(pr.out.modified, scale = 0)

# Comprobamos las variables más importantes en cada PCI
PCI <- 1
components <- pr.out$rotation[, PCI]**2
components <- sort(components, decreasing = TRUE)
cumsum(components)
length(components[cumsum(components) <= 0.8])

# Hacemos lo contrario, dada una variable vemos como se relaciona con los PCIs
components.names <- c()
threshold <- 0.8
for (var in 1:ncol(mydf.numeric)) {
  components <- pr.out$rotation[var, ]**2
  components <- sort(components, decreasing = TRUE)
  components.valids <- which(cumsum(components) <= threshold)
  components.names <- c(components.names, names(components.valids))
}
components.names <- unique(components.names)
length(components.names)


var <- 3
components <- pr.out$rotation[var, ]**2
components <- sort(components, decreasing = TRUE)
cumsum(components)
length(components[cumsum(components) <= 0.8])

#-------------------------------------------
# A partir de aquí es solo para experimentar
#-------------------------------------------

# Hacemos un producto escalar entre las variables para determinar las que están más relacionadas
var <- 3
components <- pr.out$rotation
components.mult <- abs(sweep(components, 2, components[var, ], `*`))
var.correlationated <- sort(rowSums(components.mult), decreasing = TRUE)[1:10] / max(rowSums(components.mult))
var.correlationated
rownames(components)[var]
```


```{r}
test_indices <- sample(1:nrow(mydf), 10000)
test <- mydf[test_indices, ]

train_indices <- setdiff(1:nrow(mydf), test_indices)
train <- mydf[train_indices, ]
```


```{r}
# 4

# Este problema sí podría resolverse con un algoritmo de regresión

# Dado que estamos antes un problema de clasificación binaria, el método de regresión que podemos usar es la regresión logística, especialmente una regresión logística múltiple al tener más de un predictor.

# Vamos a crear un modelo de este tipo de regresión.

lr <- glm(Class ~ ., data = mydf, family = binomial)
```

