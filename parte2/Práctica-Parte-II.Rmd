---
title: "Práctica Parte II. Aprendizaje Estadístico"
author: "Antonio Galián Gálvez, Juan Luis Serradilla Tormos"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Introducción**

En este trabajo se va a realizar un análisis de un dataset que contiene información sobre conexiones de red. El objetivo es identificar si una conexión es benigna o maligna, es decir, si se trata de un keylogger o no. Para ello, se utilizarán distintos algoritmos de machine learning y se compararán los resultados obtenidos. El cuaderno se dividirá en las siguientes secciones:

1. [Exploración del dataset y agrupación de variables](#agrupacion)
2. [Preprocesamiento de los datos](#preprocesamiento)
3. [Sampleo de los datos](#sampleo)
4. [Análisis PCA](#pca)
5. [Regresión logística](#regresion-logistica)
6. [Bagging](#bagging)
7. [Red Neuronal](#red-neuronal)
8. [Análisis de resultados](#analisis-resultados)

Antes de empezar con el trabajo tenemos que cargar las librerías necesarias, configurar el directorio de trabajo y seleccionar una semilla aleatoria para todo el cuaderno.
```{r, results = "hide", message = FALSE, warning = FALSE}
# Seleccioamos el directorio actual como directorio de trabajo
# Si estamos compilando no lo hacemos para que no de error la compilación
if (interactive()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Cargamos las librerías necesarias
library(summarytools)
library(randomForest)
library(keras)
library(magrittr)

# Seleccionamos la semilla aleatoria para todo el cuaderno
set.seed(12345)
```


Para que el cuaderno se pueda compilar rápidamente, vamos a cargar previamente los resultados que nos hagan falta. De esta forma, podremos dejar celdas sin ejecutar pero mostrando el código. 

# Exploración del dataset y agrupación de variables. {#agrupacion}
En esta sección analizaremos las variables del dataset y las agruparemos por temáticas. Se describirá el significado de cada grupo de características identificado.

```{r}
# Leemos el dataset
mydf <- read.csv("Keylogger_Detection.csv", header = TRUE, sep = ",")

# Mostramos las variables del dataset
str(mydf)
```

_Aquí debemos agrupar las variables por grupos._

# Preprocesamiento de los datos. {#preprocesamiento}
Antes de empezar con los análisis se deben preprocesar los datos. En este sección se explicarán los preprocesamientos realizados y sus razonamientos.

Lo primero de todo es visualzar el dataset completo con todas sus variables:
```{r}
# Mostramos el dataset completo
dfSummary(mydf)
```

Ahora vamos a eliminar del dataset las variables que no aportan información relevante para el análisis.
```{r}
# Guardamos una copia del dataset original para comparar posteriormente
mydf.original <- mydf

# Seleccionamos los predictores que no aportan información relevante
vars.to.remove <- c("X", "Flow.ID", "Timestamp", "Source.IP", "Destination.IP", "Source.Port", "Destination.Port")

# Eliminamos los predictores seleccionados
mydf <- mydf[, !names(mydf) %in% vars.to.remove]
```

La explicación de por qué hemos eliminado cada variable es la siguiente:

- "X" es un índice que no aporta información relevante.
- "Flow.ID" es un identificador del flujo, por lo que no es una característica como tal.
- "Timestamp" es un instante temporal, no es una característica del flujo y por tanto no aporta información relevante.
- "Source.IP" y "Destination.IP" son identificadores de los dispositivos fuente y destino, respectivamente, y no son características del flujo. No aportan información al análisis. Además, no pueden interpretarse como variables ni numéricas ni categóricas (ya que tendrían demasiados valores únicos), y no se pueden usar en un análisis siendo variables de tipo character.
- "Source.Port" y "Destination.Port" son los puertos de origen y destino, respectivamente, y no son características del flujo. No aportan información al análisis. No pueden interpretarse como variables categóricas (ya que tendrían demasiados valores únicos), y no dan información real de tipo numérico.


Una vez eliminamos las variables innecesarias, vamos a comprobar los valores nulos del dataset y ver cómo los tratamos. Empecemos viendo la cantidad de filas con nulos.
```{r}
# Buscamos las filas con valores nulos
na.coordinates <- which(is.na(mydf), arr.ind = TRUE)

# Vemos las distintas filas en las que se encuentran
na.rows <- unique(na.coordinates[, "row"])

# Calculamos la proporción respecto al número total de filas
na.prows <- length(na.rows) / nrow(mydf) * 100
paste("El porcentaje de filas con nulos es: ", na.prows, "%", sep = "")
```

Vemos que el dataset contiene muy poca cantidad de valores nulos. Por lo tanto, eliminamos las filas con valores NA.
```{r}
# Eliminamos las filas con valores nulos
mydf <- na.omit(mydf)
```

Ahora, eliminamos las filas duplicadas.
```{r, eval = FALSE}
# Eliminamos las filas duplicadas
mydf <- mydf[!duplicated(mydf), ]
```

Vamos a eliminar las columnas con desviación estándar cero. Para ello, pasaremos a numérico todas las variables que no sean claramente caracteres.
```{r, eval = FALSE}
# Pasamos a numérico las variables que les corresponde
mydf$Packet.Length.Std <- as.numeric(mydf$Packet.Length.Std)
```

Al analizar las variables vemos que hay una con una situación particular. Esta es "CWE.Flag.Count", que tiene valores "0" y "0.0". Además, los valores "0.0" no son pocos, suponen el 6% del total, más que los valores "1" de otros Flags como "FIN.Flag.Count", donde solo el 1.7% son valores 1. Esto puede deberse a que, por un error, los valores 0.0 deberían ser 1. Si fuera un error de tipografía que pasara algunos valores 0 a 0.0, no habría tantos casos. Por lo tanto, se convertirán los valores "0.0" a "1" para que siga el formato común con los otros Flags.

```{r, eval = FALSE}
# Convertimos los "0.0" a 1
mydf$CWE.Flag.Count <- ifelse(mydf$CWE.Flag.Count == "0.0", "1", mydf$CWE.Flag.Count)

# Pasamos a numérico la variable
mydf$CWE.Flag.Count <- as.numeric(mydf$CWE.Flag.Count)

# Seleccionamos las columnas numéricas
num_cols <- which(sapply(mydf, is.numeric))

# Obtenemos las columnas con desviaciones estándar nulas
constant_cols <- which(sapply(mydf[, num_cols], sd) == 0)

# Eliminamos las columnas con valores constantes
mydf <- mydf[, -constant_cols]
```

Finalmente, volvemos a hacer una revisión de las variables del dataset y asignamos los tipos correspondientes a cada variable.
```{r, eval = FALSE}
# Pasamos los que tienen valores 0, 1 a factor.
mydf$Fwd.PSH.Flags <- as.factor(mydf$Fwd.PSH.Flags)
mydf$FIN.Flag.Count <- as.factor(mydf$FIN.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
mydf$PSH.Flag.Count <- as.factor(mydf$PSH.Flag.Count)
mydf$ACK.Flag.Count <- as.factor(mydf$ACK.Flag.Count)
mydf$URG.Flag.Count <- as.factor(mydf$URG.Flag.Count)
mydf$CWE.Flag.Count <- as.factor(mydf$CWE.Flag.Count)
mydf$Class <- as.factor(mydf$Class)
```

Guardamos el dataset procesado.
```{r, eval = FALSE}
# Guardamos el dataset procesado
write.csv(mydf, "Keylogger_Detection_processed.csv", row.names = FALSE)
```

Para ver que el dataset procesado sirve para trabajar, vamos a comparar el porcentaje de valores positivos y negativos en los dos datasets.
```{r}
# Cargamos el dataset procesado
mydf <- read.csv("Keylogger_Detection_processed.csv", header = TRUE, sep = ",")

# Pasamos las variables correspondientes a factor
mydf$Protocol <- as.factor(mydf$Protocol)
mydf$Fwd.PSH.Flags <- as.factor(mydf$Fwd.PSH.Flags)
mydf$FIN.Flag.Count <- as.factor(mydf$FIN.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
mydf$PSH.Flag.Count <- as.factor(mydf$PSH.Flag.Count)
mydf$ACK.Flag.Count <- as.factor(mydf$ACK.Flag.Count)
mydf$URG.Flag.Count <- as.factor(mydf$URG.Flag.Count)
mydf$CWE.Flag.Count <- as.factor(mydf$CWE.Flag.Count)
mydf$Class <- as.factor(mydf$Class)

# Porcentaje de valores positivos y negativos en el dataset original
table(mydf.original$Class) / nrow(mydf.original) * 100

# Porcentaje de valores positivos y negativos en el dataset procesado
table(mydf$Class) / nrow(mydf) * 100
```

Vemos que la proporción de casos positivos y negativos no cambia. Por lo tanto, el dataset procesado sirve para trabajar. 

# Sampleo de los datos. {#sampleo}
Para poder trabajar con holgura a lo largo de la práctica, vamos a coger una muestra del dataset original del 20%. De esta forma, los tiempos de ejecución serán menores y podremos trabajar más rápido. Para poder trabajar con el dataset sampleado, tendremos que conseguir que la proporción de casos positivos y negativos sea la misma que en el dataset original. Para ello, se cogerán filas aleatoriamente sin reemplazo y con una probabilidad uniforme. 

```{r}
# Sampleamos el dataset
mydf.sample <- mydf[sample(nrow(mydf), nrow(mydf) * 0.2), ]

# Porcentaje de valores positivos y negativos en el dataset sampleado
table(mydf.sample$Class) / nrow(mydf.sample) * 100

# Porcentaje de valores positivos y negativos en el dataset original
table(mydf.original$Class) / nrow(mydf.original) * 100
```

Vemos que la proporción de casos positivos y negativos en el dataset sampleado es casi idéntica a la del dataset original. Por lo tanto, el dataset sampleado sirve para trabajar. Vamos a guardar dicho dataset sampleado.
```{r, eval = FALSE}
# Guardamos el dataset sampleado
write.csv(mydf.sample, "sampled.csv", row.names = FALSE)
```


# Análisis PCA. {#pca}

## Carga del dataset sampleado

Primero de todo hay que volver a cargar el dataset sampleado. Además, se deben volver a pasar a factor las variables que lo requieran.
```{r}
# Leemos el dataset preprocesado
mydf <- read.csv("sampled.csv", header = TRUE, sep = ",")

# Pasamos las variables correspondientes a factor
mydf$Protocol <- as.factor(mydf$Protocol)
mydf$Fwd.PSH.Flags <- as.factor(mydf$Fwd.PSH.Flags)
mydf$FIN.Flag.Count <- as.factor(mydf$FIN.Flag.Count)
mydf$SYN.Flag.Count <- as.factor(mydf$SYN.Flag.Count)
mydf$PSH.Flag.Count <- as.factor(mydf$PSH.Flag.Count)
mydf$ACK.Flag.Count <- as.factor(mydf$ACK.Flag.Count)
mydf$URG.Flag.Count <- as.factor(mydf$URG.Flag.Count)
mydf$CWE.Flag.Count <- as.factor(mydf$CWE.Flag.Count)
mydf$Class <- as.factor(mydf$Class)

# Nos aseguramos de que la clase positiva sea "Keylogger" asignando como clase de referencia a "Benign"
mydf$Class <- relevel(mydf$Class, ref = "Benign")
```

## Análisis del PCA

Vamos a realizar el PCA con el conjunto preprocesado. Primero, seleccionamos las columnas numéricas.
```{r}
# Seleccionamos las columnas numéricas
mydf.numeric <- mydf[, sapply(mydf, is.numeric)]
```

Una vez tenemos las columnas numéricas seleccionadas, realizamos el PCA. No hace falta estandarizar las variales previamente ya que el PCA lo hace por nosotros con el parámetro `scale = TRUE`.
```{r}
# Realizamos PCA
pr.out <- prcomp(mydf.numeric, scale = TRUE)
```

Vamos a realizar un análisis del PCA para ver qué información nos aporta. Para ello, vamos a ver primero la varianza acumulada, con el fin de determinar cuáles son las componentes principales más importantes.
```{r}
# Vemos la varianza explicada
summary(pr.out)

# Realizamos un gráfico de la varianza acumulada
pr.out.cumvar <- cumsum(pr.out$sdev^2 / sum(pr.out$sdev^2))
plot(
  pr.out.cumvar,
  xlab = "Componente principal",
  ylab = "Varianza acumulada",
  main = "Varianza acumulada explicada por las componentes principales",
  type = "b"
)
abline(h = 0.8, col = "red")

# Seleccionamos las componentes principales que expliquen al menos el 80% de la varianza
ncomp <- which(pr.out.cumvar > 0.8)[1]
ncomp
```

Vemos que necesitamos las 11 primeras componentes principales para explicar el 80% de la varianza. Solo con las 2 primeras componentes principales explicamos tan solo el 35.73% de la varianza, una cifra muy baja. Por tanto, necesitamos muchas componentes principales para explicar la varianza de los datos.

Vamos a realizar un biplot para ver cómo se relacionan las variables con las dos primeras componentes principales.
```{r, eval = FALSE}
# Realizamos un biplot solo con los vectores, no con los datos
png("biplot.png")
biplot(pr.out, scale = 0, xlim = c(-15, 15), ylim = c(-15, 15))
dev.off()
```

Como el dataset tiene demasiadas varaibles, el biplot se guarda en un .png para poder consultarse cuando se desee [biplot.png](biplot.png). Se puede ver en esta imagen que tenemos demasiados predictores y demasiados valores, no se puede sacar nada en claro. Además, como PC1 y PC2 explican solo el 35.73% de la varianza el biplot tampoco sería representativo.

Por todo esto, podemos ver que la elaboración de un modelo de clasificación que identifique conexiones de keystrokes va a ser un problema complicado. Tenemos muchos predictores y ver las relaciones entre ellos no va a ser una tarea sencilla. Si intentásemos reducir la dimensionalidad del problema con PCA, como mucho podríamos reducir a 11 dimensiones, lo cual sigue siendo una gran cantidad de variables.


# Regresión logística. {#regresion-logistica}
Vamos a intentar predecir si una conexión es benigna o maligna mediante un modelo de regresión logística. Realizaremos unas predicciones con el resultado del modelo y veremos cómo funciona para el dataset. No usaremos conjunto de prueba ni de test, ya que simplemente queremos ver cómo de eficaz es con nuestro conjunto de datos.

```{r}
# Creamos el modelo de regresión logística
logr <- glm(Class ~ ., data = mydf, family = binomial)

# Vemos un resumen de logr
summary(logr)

# Vemos los coeficientes que han generado aliasing en el modelo
alias.info <- alias(logr)
alias.matrix <- as.matrix(alias.info$Complete)
alias.matrix <- abs(alias.matrix[, apply(alias.matrix, 2, sd) != 0])
alias.matrix <- round(alias.matrix)
alias.matrix
```

Vemos que hay variables que han generado aliasing en el modelo, lo que significa que están perfectamente correlacionadas con otras variables (variables de filas con variables de columnas). El modelo de regresión logística directamente no tendrá en cuenta estas variables. 

Una vez tenemos el modelo creado, realizamos unas predicciones. Con estas predicciones se harán unas tablas de confusión para ver cómo se comporta el modelo.
```{r}
# Realizamos las predicciones
logr.probs <- predict(logr, type = "response")
logr.pred <- ifelse(logr.probs > 0.5, "Benign", "Keylogger")
logr.table <- table(logr.pred, mydf$Class)
dimnames(logr.table) <- list(
  Predict = c("Benign", "Keylogger"),
  Real = c("Benign", "Keylogger")
)

# Realizamos las tablas de confusión
addmargins(round(prop.table(logr.table) * 100, 2))

# Hacemos una suma por columnas para ver la precisión
addmargins(round(prop.table(logr.table, 2) * 100, 1), 1)

# Hacemos una suma por filas para ver la sensibilidad
addmargins(round(prop.table(logr.table, 1) * 100, 1), 2)
```

Analizando la tabla de confusión vemos que el modelo no es capaz de predecir correctamente si una conexión es benigna o maligna.

- De las conexiones que son benignas, el modelo solo es capaz de predecir correctamente el 29.2%. 
- De las conexiones que son keylogger, el modelo predice correctamente el 55.1%.
- De las conexiones que se han predicho como benignas, solo el 48.5% eran realmente benignas, dejando una tasa de falso positivo del 51.5%.
- De las conexiones que se han predicho como keylogger, solo el 65.0% eran realmente keylogger, dejando una tasa de falso negativo del 35.0%.
  
Con este análisis podemos concluir que el modelo es nefasto prediciendo conexiones benignas. Además, tampoco es capaz de predecir correctamente las conexiones malignas, acertando en un 55.1% de las veces. Por tanto, el modelo de regresión logística no es adecuado para predecir si una conexión es benigna o maligna.


# Bagging. {#bagging}



# Red Neuronal. {#red-neuronal}

Dado que el número de datos que tenemos disponibles para entrenar el modelo elegido puede considerarse relativamente alto, una buena opción a utilizar es la red neuronal, la cual mejora su rendimiento cuando dispone de muchos datos. Además, tanto la alta dimensionalidad, como la aparente complejidad en las relaciones entre la variable de salida y las de entrada, pueden ser idóneas para un modelo especializado en captar relaciones no lineales, como es el de la red neuronal. En este modelo, son tres los posibles hiperparámetros entre los que elegir, siendo los dos primeros relativos a la propia arquitectura de la red:

- Neuronas en la primera capa. Se elegirá entre los valores 16, 32 y 1024. A más neuronas en la capa, el modelo amplía en complejidad, lo que puede ayudar con la no linealidad de nuestro problema.

- Neuronas en la segunda capa. Los valores posibles son 0, 16, 32 y 1024. El primer valor indica que esta capa no es incluida directamente en el modelo, por lo que solo se construiría la red con la primera capa definida anteriormente.

- Número de épocas. Es el número de veces que, durante el proceso de entrenamiento, la red ha usado el conjunto total de datos disponibles para entrenar. Cuanto mayor es, más permite al modelo ajustarse a dichos datos y minimizar el error de entrenamiento. Sin embargo, esto conlleva un inconveniente, ya que si el modelo se ajusta demasiado, puede ocurrir overfitting y que no generalice bien a los datos de test. Es por ello que resulta necesario controlar el número de épocas como hiperparámetro. 

Hay otros aspectos de la arquitectura de la red a tener en cuenta. Las funciones de activación que se tendrán en todas las neuronas será la misma: la función 'relu', la cual dota de no linealidad al modelo. También, debido a la alta dimensionalidad de nuestro problema, se ha aplicado regularización a la red añadiendole una capa de regularización 'Dropout' de tasa 0.2 tras cada capa oculta. Para la capa de salida, se usarán dos neuronas, una por cada clase distinta, con función de activación 'softmax', que produce como salida la probabilidad de que un vector de entrada pertenezca a su correspondiente clase.

En cuanto a la función de pérdida, se usa la "categorical_crossentropy", ideal para problemas de clasificación categórica, y el optimizador usado sera "Adam", el cual maneja el parámetro de learning rate por sí mismo, por lo que no es necesario tratar este último como un hiperparámetro que variar.


# Análisis de resultados. {#analisis-resultados}