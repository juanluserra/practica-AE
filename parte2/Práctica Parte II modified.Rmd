---
title: "Práctica Parte II. Aprendizaje Estadístico"
author: "Antonio Galián Gálvez, Juan Luis Serradilla Tormos"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Introducción**

En este trabajo se va a realizar un análisis de un dataset que contiene información sobre conexiones de red. El objetivo es identificar si una conexión es benigna o maligna, es decir, si es un keylogger o no. Para ello, se utilizarán distintos algoritmos de machine learning y se compararán los resultados obtenidos. El cuaderno se dividirá en las siguientes secciones:

1. [Exploración del dataset y agrupación de variables](#agrupacion)
2. [Preprocesamiento de los datos](#preprocesamiento)
3. [Sampleo de los datos](#sampleo)
4. [Análisis PCA](#pca)
5. [Regresión logística](#regresion-logistica)
6. [Bagging](#bagging)
7. [Red Neuronal](#red-neuronal)
8. [Análisis de resultados](#analisis-resultados)

Antes de empezar con el trabajo tenemos que cargar las librerías necesarias, configurar el directorio de trabajo y seleccionar una semilla aleatoria para todo el cuaderno.
```{r, results = "hide", message = FALSE, warning = FALSE}
# Seleccioamos el directorio actual como directorio de trabajo
# Si estamos compilando no lo hacemos para que no de error la compilación
if (interactive()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Cargamos las librerías necesarias
library(summarytools)
library(randomForest)
library(keras)
library(magrittr)

# Seleccionamos la semilla aleatoria para todo el cuaderno
set.seed(12345)
```


# Exploración del dataset y agrupación de variables. {#agrupacion}
En esta sección analizaremos las variables del dataset y las agruparemos por temáticas. Se describirá el significado de cada grupo de características identificado.

```{r}
# Leemos el dataset
mydf <- read.csv("Keylogger_Detection.csv", header = TRUE, sep = ",")

# Mostramos las variables del dataset
str(mydf)
```

_Aquí debemos agrupar las variables por grupos._

# Preprocesamiento de los datos. {#preprocesamiento}
Antes de empezar con los análisis se deben preprocesar los datos. En este sección se explicarán los preprocesamientos realizados y sus razonamientos.

Lo primero de todo es eliminar del dataset las variables que no aportan información relevante para el análisis.
```{r}
# Seleccionamos los predictores que no aportan información relevante
vars.to.remove <- c("X", "Flow.ID", "Timestamp", "Source.IP", "Destination.IP", "Source.Port", "Destination.Port")

# Eliminamos los predictores seleccionados
mydf <- mydf[, !names(mydf) %in% vars.to.remove]
```

La explicación de por qué hemos eliminado cada variable es la siguiente:

- "X" es un índice que no aporta información relevante.
- "Flow.ID" es un identificador del flujo, por lo que no es una característica como tal.
- "Timestamp" es un instante temporal, no es una característica del flujo y por tanto no aporta información relevante.
- "Source.IP" y "Destination.IP" son identificadores de los dispositivos fuente y destino, respectivamente, y no son características del flujo. No aportan información al análisis. Además, no pueden interpretarse como variables ni numéricas ni categóricas (ya que tendrían demasiados valores únicos), y no se puede realizar un análisis siendo variables de tipo character.
- "Source.Port" y "Destination.Port" son los puertos de origen y destino, respectivamente, y no son características del flujo. No aportan información al análisis. No pueden interpretarse como variables categóricas (ya que tendrían demasiados valores únicos), y no dan información real de tipo numérico.


Una vez eliminamos las variables innecesarias, vamos a comprobar los valores nulos del dataset y ver cómo los tratamos.

# Sampleo de los datos. {#sampleo}


# Análisis PCA. {#pca}


# Regresión logística. {#regresion-logistica}


# Bagging. {#bagging}


# Red Neuronal. {#red-neuronal}


# Análisis de resultados. {#analisis-resultados}